"""
Whisper Processor - Handles audio processing and transcription
ä½¿ç”¨ OpenAI Whisper å®ç°ï¼Œæ”¯æŒè¯çº§æ—¶é—´æˆ³å’Œç¼“å­˜
"""

from PySide6.QtCore import QObject, Signal
from pathlib import Path
import tempfile
import time
import os
import hashlib
import pickle
from datetime import timedelta
import subprocess
from pydub import AudioSegment
from utils.srt_utils import merge_srt_files
from utils.fcpxml_generator import generate_fcpxml
from utils.paths import setup_whisper_cache, get_models_dir
from utils.model_loader import format_bytes

# â­ åœ¨æ¨¡å—åŠ è½½æ—¶å°±å¯¼å…¥ whisperï¼Œé¿å…åœ¨ QThread ä¸­å¯¼å…¥å¯¼è‡´å´©æºƒ
print("ğŸ”§ é¢„åŠ è½½ whisperï¼ˆé¿å…çº¿ç¨‹å´©æºƒï¼‰...")
try:
    import whisper
    print("âœ… whisper é¢„åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"âŒ whisper é¢„åŠ è½½å¤±è´¥: {e}")
    whisper = None


class WhisperProcessor(QObject):
    progress = Signal(float)
    status = Signal(str)
    output = Signal(str)
    batch_info = Signal(int, int, int, str)  # current, total, percentage, remaining
    finished = Signal(str, str)  # srt_path, fcpxml_path
    error = Signal(str)
    
    def __init__(self, data):
        super().__init__()
        print("ğŸ”§ WhisperProcessor.__init__() å¼€å§‹")
        print(f"   data: {data}")
        
        self.data = data
        self.model = None
        self._download_start_time = None
        self._last_download_update = 0
        
        # ç¼“å­˜å¼€å…³ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        self.enable_cache = data.get('enable_cache', True)
        cache_status = 'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'
        print(f"   ç¼“å­˜å¼€å…³: {cache_status}")
        
        print("   åˆ›å»ºç¼“å­˜ç›®å½•...")
        # ç¼“å­˜ç›®å½•ï¼ˆä¸æ™ºèƒ½åˆ†å‰²å…±äº«ï¼‰
        self.cache_dir = Path.home() / 'Videos' / 'pyvideotrans' / 'get_srt_zimu' / 'whisper_cache'
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        print(f"   âœ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print("ğŸ”§ WhisperProcessor.__init__() å®Œæˆ\n")
        
    def _setup_download_progress_hook(self):
        """Setup hooks to monitor download progress"""
        import urllib.request
        
        # Store original urlretrieve
        original_urlretrieve = urllib.request.urlretrieve
        
        def progress_hook(block_num, block_size, total_size):
            downloaded = block_num * block_size
            
            # Initialize start time
            if self._download_start_time is None:
                self._download_start_time = time.time()
                self.output.emit("â¬‡ï¸  Starting download...\n")
            
            if total_size > 0:
                percentage = min((downloaded / total_size) * 100, 100)
                
                # Update every 5% or every 10 seconds
                current_time = time.time()
                if percentage - self._last_download_update >= 5 or \
                   current_time - self._download_start_time > 10:
                    
                    elapsed = current_time - self._download_start_time
                    speed = downloaded / elapsed if elapsed > 0 else 0
                    
                    self.output.emit(
                        f"ğŸ“¥ Downloaded: {format_bytes(downloaded)} / {format_bytes(total_size)} "
                        f"({percentage:.1f}%) - Speed: {format_bytes(int(speed))}/s\n"
                    )
                    self._last_download_update = percentage
        
        def custom_urlretrieve(url, filename, reporthook=None, data=None):
            """Custom urlretrieve with progress reporting"""
            return original_urlretrieve(url, filename, progress_hook, data)
        
        # Monkey patch urllib
        urllib.request.urlretrieve = custom_urlretrieve
        
    def _get_best_device(self):
        """Detect and return the best available device for processing"""
        try:
            import torch
            if torch.cuda.is_available():
                print("   æ£€æµ‹è®¾å¤‡ï¼šCUDA GPU å¯ç”¨")
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                # âš ï¸ MPS è®¾å¤‡åœ¨è¯çº§æ—¶é—´æˆ³æ¨¡å¼ä¸‹æœ‰å…¼å®¹æ€§é—®é¢˜ï¼ˆä¸æ”¯æŒ float64ï¼‰
                # å› æ­¤ä½¿ç”¨ CPU ä»¥ç¡®ä¿è¯çº§æ—¶é—´æˆ³åŠŸèƒ½æ­£å¸¸å·¥ä½œ
                print("   æ£€æµ‹è®¾å¤‡ï¼šApple Silicon MPS å¯ç”¨ï¼Œä½†è¯çº§æ—¶é—´æˆ³éœ€è¦ CPU")
                print("   ğŸ“ æç¤ºï¼šä½¿ç”¨ CPU æ¨¡å¼ä»¥æ”¯æŒè¯çº§æ—¶é—´æˆ³")
                return "cpu"
            else:
                print("   æ£€æµ‹è®¾å¤‡ï¼šä½¿ç”¨ CPU")
                return "cpu"
        except ImportError:
            print("   æ£€æµ‹è®¾å¤‡ï¼štorch æœªå®‰è£…ï¼Œä½¿ç”¨ CPU")
            return "cpu"
        
    def process(self):
        """Main processing function"""
        print("\n" + "=" * 60)
        print("âš™ï¸  process() æ–¹æ³•è¢«è°ƒç”¨")
        print("=" * 60)
        
        try:
            print("å°è¯•å‘é€ç¬¬ä¸€ä¸ªä¿¡å·...")
            self.output.emit("\nğŸš€ å¼€å§‹å¤„ç†æµç¨‹...\n\n")
            print("âœ“ ç¬¬ä¸€ä¸ªä¿¡å·å‘é€æˆåŠŸ")
            
            # Setup custom cache directory for models
            print("å‡†å¤‡å‘é€ Step 1 ä¿¡å·...")
            self.output.emit("ğŸ“‚ Step 1: è®¾ç½®ç›®å½•...\n")
            print("âœ“ Step 1 ä¿¡å·å‘é€æˆåŠŸ")
            
            print("è°ƒç”¨ setup_whisper_cache()...")
            models_dir = setup_whisper_cache()
            print(f"âœ“ setup_whisper_cache() è¿”å›: {models_dir}")
            
            print("å‘é€ models ç›®å½•ä¿¡æ¯...")
            self.output.emit(f"   âœ“ Models ç›®å½•: {models_dir}\n")
            print("å‘é€ cache ç›®å½•ä¿¡æ¯...")
            self.output.emit(f"   âœ“ Cache ç›®å½•: {self.cache_dir}\n\n")
            print("âœ“ Step 1 å®Œæˆ")
            
            print("è·å–æ¨¡å‹ä¿¡æ¯...")
            model_name = self.data['model']
            model_display = self.data.get('model_display', model_name)
            print(f"   model_name: {model_name}")
            print(f"   model_display: {model_display}")
            
            # Show model information
            print("å‘é€æ¨¡å‹ä¿¡æ¯åˆ° UI...")
            self.output.emit("=" * 60 + "\n")
            self.output.emit(f"ğŸ¯ Model: {model_display} (OpenAI Whisper)\n")
            self.output.emit(f"ğŸ“ Models directory: {models_dir}\n")
            self.output.emit(f"ğŸ’¾ Cache directory: {self.cache_dir}\n")
            self.output.emit("=" * 60 + "\n\n")
            print("âœ“ æ¨¡å‹ä¿¡æ¯å‘é€å®Œæˆ")
            
            # Detect best available device
            print("\nStep 2: æ£€æµ‹è®¾å¤‡...")
            self.output.emit("ğŸ–¥ï¸  Step 2: æ£€æµ‹è®¡ç®—è®¾å¤‡...\n")
            print("è°ƒç”¨ _get_best_device()...")
            device = self._get_best_device()
            self.device = device  # ä¿å­˜ä¸ºå®ä¾‹å˜é‡
            print(f"âœ“ æ£€æµ‹åˆ°è®¾å¤‡: {device}")
            self.output.emit(f"   âœ“ æ£€æµ‹åˆ°è®¾å¤‡: {device}\n\n")
            print("âœ“ Step 2 å®Œæˆ")
            
            print("\nStep 3: åŠ è½½æ¨¡å‹...")
            self.status.emit(f"Loading OpenAI Whisper {model_display} model...")
            self.output.emit(f"âš™ï¸  Step 3: åŠ è½½ OpenAI Whisper æ¨¡å‹...\n")
            self.output.emit(f"   æ¨¡å‹: {model_display}\n")
            self.output.emit(f"   è®¾å¤‡: {device}\n")
            self.output.emit("   æ­£åœ¨åŠ è½½...\n\n")
            
            # Load OpenAI Whisper model
            try:
                print("æ£€æŸ¥ whisper...")
                self.output.emit("   ğŸ“¥ ä½¿ç”¨é¢„åŠ è½½çš„ whisper...\n")
                
                if whisper is None:
                    raise ImportError("whisper æœªèƒ½é¢„åŠ è½½")
                
                print("âœ“ whisper å¯ç”¨")
                self.output.emit("   âœ“ whisper å·²å°±ç»ª\n")
                
                print(f"åŠ è½½æ¨¡å‹: {model_name}")
                print(f"   device: {device}")
                print(f"   download_root: {models_dir}")
                
                self.output.emit(f"   ğŸ“¥ åŠ è½½æ¨¡å‹ {model_name}...\n")
                self.output.emit(f"   ï¼ˆé¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰\n")
                
                print("åˆ›å»º Whisper æ¨¡å‹å®ä¾‹...")
                # OpenAI Whisper loads model to the specified device
                self.model = whisper.load_model(model_name, device=device, download_root=str(models_dir))
                print("âœ“ Whisper æ¨¡å‹åˆ›å»ºæˆåŠŸ")
                
                self.output.emit(f"\nâœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\n")
                self.output.emit(f"   Device: {device}\n\n")
                
                # Show device info
                if device == "cuda":
                    self.output.emit("âœ“ Using NVIDIA GPU acceleration (CUDA)\n")
                elif device == "mps":
                    self.output.emit("âœ“ Using Apple Silicon GPU acceleration (MPS)\n")
                else:
                    self.output.emit("â„¹ Using CPU\n")
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸º MPS é™åˆ¶è€Œä½¿ç”¨ CPU
                    try:
                        import torch
                        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                            self.output.emit("ğŸ“ æ³¨æ„ï¼šApple Silicon æ£€æµ‹åˆ°ï¼Œä½†è¯çº§æ—¶é—´æˆ³éœ€è¦ CPU\n")
                            self.output.emit("   åŸå› ï¼šMPS ä¸æ”¯æŒ float64ï¼ˆDTW ç®—æ³•éœ€è¦ï¼‰\n")
                            self.output.emit("   æ€§èƒ½ï¼šApple Silicon CPU ä¾ç„¶å¾ˆå¿«ï¼âš¡\n")
                    except:
                        pass
                    
            except Exception as e:
                import traceback
                error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥!\n\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯¦ç»†å †æ ˆ:\n{traceback.format_exc()}"
                self.output.emit(error_msg)
                self.error.emit(error_msg)
                return
            
            # æ£€æŸ¥ç¼“å­˜
            try:
                self.output.emit("\nğŸ” Step 4: æ£€æŸ¥ç¼“å­˜...\n")
                self.output.emit(f"   è§†é¢‘æ–‡ä»¶: {self.data['file_path']}\n")
                cache_status = 'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'
                self.output.emit(f"   ç¼“å­˜å¼€å…³: {cache_status}\n")
                
                all_words = None
                detected_language = None
                cached_data = None
                
                # åªæœ‰å¯ç”¨ç¼“å­˜æ—¶æ‰æ£€æŸ¥å’ŒåŠ è½½
                if self.enable_cache:
                    cache_key = self._get_cache_key(self.data['file_path'])
                    self.output.emit(f"   ç¼“å­˜é”®: {cache_key[:16]}... (SHA256)\n")
                    cached_data = self._load_cache(cache_key)
                else:
                    self.output.emit("   âš ï¸  ç¼“å­˜å·²ç¦ç”¨ï¼Œå°†é‡æ–°è¯†åˆ«\n")
                
                if cached_data:
                    self.output.emit("   âœ… æ‰¾åˆ°ç¼“å­˜ï¼\n")
                    all_words = cached_data['all_words']
                    detected_language = cached_data['language']
                    self.output.emit(f"   ğŸ“Š ä»ç¼“å­˜åŠ è½½: {len(all_words)} ä¸ªè¯\n")
                    self.output.emit(f"   ğŸŒ æ£€æµ‹è¯­è¨€: {detected_language}\n\n")
                else:
                    self.output.emit("   âŒ æœªæ‰¾åˆ°ç¼“å­˜\n\n")
                    self.output.emit("ğŸµ Step 5: å¼€å§‹è¯­éŸ³è¯†åˆ«...\n\n")
                    
                    # Convert audio to WAV
                    self.status.emit("Converting audio to WAV format...")
                    self.output.emit("   Step 5.1: è½¬æ¢éŸ³é¢‘æ ¼å¼...\n")
                    self.output.emit(f"   æºæ–‡ä»¶: {self.data['file_path']}\n")
                    
                    try:
                        wav_path = self._convert_to_wav(self.data['file_path'])
                        self.output.emit(f"   âœ… è½¬æ¢å®Œæˆ: {wav_path}\n\n")
                    except Exception as e:
                        import traceback
                        error_msg = f"âŒ éŸ³é¢‘è½¬æ¢å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
                        self.output.emit(error_msg)
                        self.error.emit(error_msg)
                        return
                    
                    # ä½¿ç”¨ OpenAI Whisper è¿›è¡Œè½¬å½•ï¼ˆè·å–è¯çº§æ—¶é—´æˆ³ï¼‰
                    self.status.emit("Generating AI subtitles with word timestamps...")
                    self.output.emit("   Step 5.2: å¼€å§‹è¯­éŸ³è¯†åˆ«ï¼ˆè¯çº§æ—¶é—´æˆ³ï¼‰\n")
                    self.output.emit("   â³ æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...\n\n")
                    
                    try:
                        start_time = time.time()
                        self.output.emit("   ğŸ¤ è°ƒç”¨ _transcribe_with_word_timestamps()...\n")
                        all_words, detected_language = self._transcribe_with_word_timestamps(wav_path)
                        transcribe_time = time.time() - start_time
                        
                        if not all_words:
                            self.error.emit("æœªæ£€æµ‹åˆ°ä»»ä½•è¯­éŸ³å†…å®¹")
                            return
                        
                        self.output.emit(f"\nâœ… è¯†åˆ«å®Œæˆï¼\n")
                        self.output.emit(f"   è€—æ—¶: {transcribe_time:.1f}ç§’\n")
                        self.output.emit(f"   æ£€æµ‹è¯­è¨€: {detected_language}\n")
                        self.output.emit(f"   è¯æ•°: {len(all_words)}\n\n")
                        
                        # ä¿å­˜ç¼“å­˜ï¼ˆä»…åœ¨å¯ç”¨ç¼“å­˜æ—¶ï¼‰
                        if self.enable_cache:
                            self._save_cache(cache_key, all_words, detected_language)
                        else:
                            self.output.emit(f"ğŸ’¡ æç¤º: ç¼“å­˜å·²ç¦ç”¨ï¼Œæœªä¿å­˜è¯çº§æ—¶é—´æˆ³\n")
                        
                    except Exception as e:
                        import traceback
                        self.error.emit(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
                        return
                        
            except Exception as e:
                import traceback
                self.error.emit(f"å¤„ç†å¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
                return
            
            # ä»è¯çº§æ—¶é—´æˆ³ç”Ÿæˆ SRT æ–‡ä»¶
            self.status.emit("Generating subtitle files...")
            self.output.emit("ğŸ“ ä»è¯çº§æ—¶é—´æˆ³ç”Ÿæˆå­—å¹•æ–‡ä»¶...\n")
            srt_files = self._generate_srt_from_words(all_words)
            
            # Merge SRT files
            self.status.emit("Merging subtitle files...")
            self.output.emit("\nMerging subtitle files...\n")
            merged_srt_path = merge_srt_files(srt_files, self.data['project_name'])
            
            # Generate FCPXML
            self.status.emit("Generating FCPXML file...")
            self.output.emit("Generating FCPXML file...\n")
            fcpxml_path = generate_fcpxml(
                merged_srt_path,
                self.data['fps'],
                self.data['project_name'],
                self.data['language']
            )
            
            self.output.emit("\nâœ“ All processing completed!\n")
            self.finished.emit(merged_srt_path, fcpxml_path)
            
        except Exception as e:
            import traceback
            error_msg = f"âŒ process() æ–¹æ³•å¼‚å¸¸: {str(e)}\n\n{traceback.format_exc()}"
            print(error_msg)
            try:
                self.error.emit(error_msg)
            except:
                print("âš ï¸  æ— æ³•å‘é€é”™è¯¯ä¿¡å·")
            
    def _convert_to_wav(self, audio_path):
        """Convert audio/video to 16kHz WAV format"""
        self.output.emit(f"      å°è¯•ä½¿ç”¨ pydub è½¬æ¢...\n")
        try:
            # pydub can handle most formats including video files (extracts audio)
            audio = AudioSegment.from_file(audio_path)
            self.output.emit(f"      âœ“ æ–‡ä»¶è¯»å–æˆåŠŸ\n")
            
            audio = audio.set_frame_rate(16000).set_channels(1)
            self.output.emit(f"      âœ“ è½¬æ¢ä¸º 16kHz å•å£°é“\n")
            
            wav_path = Path(tempfile.gettempdir()) / f"{self.data['project_name']}.wav"
            self.output.emit(f"      âœ“ å¯¼å‡ºåˆ°: {wav_path}\n")
            
            audio.export(str(wav_path), format="wav")
            return str(wav_path)
        except Exception as e:
            self.output.emit(f"      âš ï¸  pydub å¤±è´¥: {str(e)}\n")
            self.output.emit(f"      å°è¯•ä½¿ç”¨ ffmpeg...\n")
            
            # If pydub fails, try using ffmpeg directly (for video files)
            wav_path = Path(tempfile.gettempdir()) / f"{self.data['project_name']}.wav"
            try:
                subprocess.run([
                    'ffmpeg', '-i', audio_path,
                    '-vn',  # No video
                    '-acodec', 'pcm_s16le',
                    '-ar', '16000',
                    '-ac', '1',
                    str(wav_path),
                    '-y'  # Overwrite
                ], check=True, capture_output=True)
                self.output.emit(f"      âœ“ ffmpeg è½¬æ¢æˆåŠŸ\n")
                return str(wav_path)
            except Exception as ffmpeg_error:
                raise Exception(f"Failed to convert audio: {str(e)}\nFFmpeg error: {str(ffmpeg_error)}")
        
    def _split_audio(self, wav_path):
        """Split audio into 10-minute segments if needed"""
        audio = AudioSegment.from_wav(wav_path)
        duration_ms = len(audio)
        segment_duration_ms = 10 * 60 * 1000  # 10 minutes
        
        if duration_ms <= segment_duration_ms:
            return [wav_path]
        
        segments = []
        num_segments = (duration_ms + segment_duration_ms - 1) // segment_duration_ms
        
        for i in range(num_segments):
            start = i * segment_duration_ms
            end = min((i + 1) * segment_duration_ms, duration_ms)
            segment = audio[start:end]
            
            segment_path = Path(tempfile.gettempdir()) / f"{self.data['project_name']}_segment_{i}.wav"
            segment.export(str(segment_path), format="wav")
            segments.append(str(segment_path))
            
        return segments
        
    def _transcribe_with_word_timestamps(self, audio_path):
        """ä½¿ç”¨ OpenAI Whisper è¿›è¡Œè½¬å½•ï¼Œè·å–è¯çº§æ—¶é—´æˆ³"""
        try:
            language_code = self.data['language_code']
            
            # Set initial prompt for Chinese
            initial_prompt = None
            if language_code == "zh":
                if self.data['language'] == "Chinese Simplified":
                    initial_prompt = "ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„å¥å­"
                else:
                    initial_prompt = "ä»¥ä¸‹æ˜¯æ™®é€šè©±çš„å¥å­"
            
            self.output.emit(f"   è¯­è¨€ä»£ç : {language_code}\n")
            if initial_prompt:
                self.output.emit(f"   åˆå§‹æç¤º: {initial_prompt}\n")
            self.output.emit("\n   å¼€å§‹è½¬å½•...\n")
            
            # æ ¹æ®è®¾å¤‡é€‰æ‹©ç²¾åº¦
            device = getattr(self, 'device', 'cpu')
            use_fp16 = (device == 'cuda')  # ä»…åœ¨ CUDA ä¸Šä½¿ç”¨ FP16
            self.output.emit(f"   ç²¾åº¦è®¾ç½®: {'FP16' if use_fp16 else 'FP32'}\n")
            
            # ä½¿ç”¨ OpenAI Whisper è½¬å½•
            result = self.model.transcribe(
                audio_path,
                language=language_code if language_code else None,
                initial_prompt=initial_prompt,
                word_timestamps=True,  # â­ å¯ç”¨è¯çº§æ—¶é—´æˆ³
                fp16=use_fp16,  # CPU ä½¿ç”¨ FP32ï¼ŒCUDA ä½¿ç”¨ FP16
                verbose=False
            )
            
        except Exception as e:
            self.output.emit(f"\nâŒ è½¬å½•åˆå§‹åŒ–å¤±è´¥: {str(e)}\n")
            raise
        
        # æ”¶é›†æ‰€æœ‰è¯å’Œæ–‡æœ¬
        try:
            all_words = []
            full_text = []
            segment_count = 0
            
            self.output.emit("   å¼€å§‹æ”¶é›†è¯çº§æ—¶é—´æˆ³...\n")
            
            # OpenAI Whisper returns segments in result['segments']
            segments = result.get('segments', [])
            detected_language = result.get('language', language_code)
            
            for segment in segments:
                segment_count += 1
                if segment_count % 10 == 0:
                    self.output.emit(f"   å¤„ç†ç‰‡æ®µ: {segment_count}...\n")
                
                # æ”¶é›†æ–‡æœ¬
                if 'text' in segment:
                    full_text.append(segment['text'].strip())
                
                # æ”¶é›†è¯çº§æ—¶é—´æˆ³
                if 'words' in segment:
                    for word in segment['words']:
                        all_words.append({
                            'word': word.get('word', ''),
                            'start': word.get('start', 0),
                            'end': word.get('end', 0)
                        })
            
            self.output.emit(f"   æ”¶é›†å®Œæˆï¼š{segment_count} ä¸ªç‰‡æ®µ\n")
            
            # è¾“å‡ºè¯†åˆ«çš„æ–‡æœ¬
            if full_text:
                self.output.emit("\nğŸ“„ è¯†åˆ«æ–‡æœ¬é¢„è§ˆ:\n")
                preview = ' '.join(full_text[:10])  # æ˜¾ç¤ºå‰10æ®µ
                if len(full_text) > 10:
                    preview += "..."
                self.output.emit(f"   {preview}\n")
            
            return all_words, detected_language
            
        except Exception as e:
            self.output.emit(f"\nâŒ æ”¶é›†æ•°æ®å¤±è´¥: {str(e)}\n")
            raise
        
    def _generate_srt_from_words(self, all_words):
        """ä»è¯çº§æ—¶é—´æˆ³ç”Ÿæˆ SRT æ–‡ä»¶"""
        # ä½¿ç”¨ç®€å•è§„åˆ™å°†è¯ç»„åˆæˆå¥å­
        subtitles = []
        current_words = []
        current_start = None
        
        max_words = 15  # æ¯æ¡å­—å¹•æœ€å¤š 15 ä¸ªè¯
        max_duration = 5.0  # æ¯æ¡å­—å¹•æœ€å¤š 5 ç§’
        sentence_ends = {'.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ'}
        
        for i, word in enumerate(all_words):
            if current_start is None:
                current_start = word['start']
            
            current_words.append(word)
            duration = word['end'] - current_start
            word_text = word['word'].strip()
            
            should_split = False
            
            # å¥å­ç»“æŸ
            if word_text and word_text[-1] in sentence_ends:
                should_split = True
            # è¶…è¿‡é™åˆ¶
            elif duration >= max_duration or len(current_words) >= max_words:
                should_split = True
            
            if should_split and current_words:
                subtitle = {
                    'start': current_start,
                    'end': current_words[-1]['end'],
                    'text': ''.join([w['word'] for w in current_words]).strip()
                }
                subtitles.append(subtitle)
                current_words = []
                current_start = None
        
        # å¤„ç†å‰©ä½™çš„è¯
        if current_words:
            subtitle = {
                'start': current_start,
                'end': current_words[-1]['end'],
                'text': ''.join([w['word'] for w in current_words]).strip()
            }
            subtitles.append(subtitle)
        
        # ç”Ÿæˆä¸´æ—¶ SRT æ–‡ä»¶
        srt_path = Path(tempfile.gettempdir()) / f"{self.data['project_name']}_word_based.srt"
        self._write_srt_from_subtitles(subtitles, str(srt_path))
        
        self.output.emit(f"âœ… ç”Ÿæˆ {len(subtitles)} æ¡å­—å¹•\n")
        
        return [str(srt_path)]
    
    def _write_srt_from_subtitles(self, subtitles, output_path):
        """å°†å­—å¹•åˆ—è¡¨å†™å…¥ SRT æ–‡ä»¶"""
        with open(output_path, 'w', encoding='utf-8') as f:
            for i, sub in enumerate(subtitles, 1):
                f.write(f"{i}\n")
                f.write(f"{self._format_timestamp(sub['start'])} --> {self._format_timestamp(sub['end'])}\n")
                f.write(f"{sub['text']}\n\n")
    
    def _get_cache_key(self, file_path):
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆä¸æ™ºèƒ½åˆ†å‰²å…±äº«ï¼‰"""
        try:
            hash_obj = hashlib.sha256()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(8192), b''):
                    hash_obj.update(chunk)
            return hash_obj.hexdigest()
        except Exception as e:
            self.output.emit(f'âš ï¸ è®¡ç®—å“ˆå¸Œå€¼å¤±è´¥: {str(e)}\n')
            return None
    
    def _save_cache(self, cache_key, all_words, language):
        """ä¿å­˜ç¼“å­˜ï¼ˆä¸æ™ºèƒ½åˆ†å‰²å…±äº«ï¼‰"""
        if not cache_key:
            return
        
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            cache_data = {
                'all_words': all_words,
                'language': language,
                'timestamp': time.time()
            }
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            self.output.emit(f'ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {cache_file.name}\n')
            self.output.emit(f'ğŸ“ æç¤º: æ™ºèƒ½åˆ†å‰²åŠŸèƒ½å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤ç¼“å­˜ï¼\n')
        except Exception as e:
            self.output.emit(f'âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}\n')
    
    def _load_cache(self, cache_key):
        """åŠ è½½ç¼“å­˜ï¼ˆä¸æ™ºèƒ½åˆ†å‰²å…±äº«ï¼‰"""
        if not cache_key:
            return None
        
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if not cache_file.exists():
            return None
        
        try:
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            return cache_data
        except Exception as e:
            self.output.emit(f'âš ï¸ è¯»å–ç¼“å­˜å¤±è´¥: {str(e)}\n')
            return None
                
    def _format_timestamp(self, seconds):
        """Format timestamp for SRT format"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

