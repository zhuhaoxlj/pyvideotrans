# LLMæ™ºèƒ½å­—å¹•æ–­å¥ä¼˜åŒ– - åŸºäºè¯­ä¹‰ç†è§£
def openwin():
    import json
    from pathlib import Path

    from PySide6.QtCore import QThread, Signal, QUrl
    from PySide6.QtGui import QDesktopServices
    from PySide6.QtWidgets import QFileDialog

    from videotrans.configure import config
    from videotrans.util import tools
    
    RESULT_DIR = config.HOME_DIR + "/SmartSplit"
    Path(RESULT_DIR).mkdir(exist_ok=True)
    
    # ç¼“å­˜ç›®å½•
    CACHE_DIR = Path(config.HOME_DIR) / "whisper_cache"
    CACHE_DIR.mkdir(exist_ok=True)

    class LLMSplitThread(QThread):
        uito = Signal(str)

        def __init__(self, *, parent=None, video_file=None, language='en', model_size='large-v3-turbo', 
                     max_duration=5.0, max_words=15, device='cpu', existing_srt=None,
                     llm_provider='openai', llm_api_key='', llm_model='gpt-4o-mini', llm_base_url=''):
            super().__init__(parent=parent)
            self.video_file = video_file
            self.language = language
            self.model_size = model_size
            self.max_duration = max_duration
            self.max_words = max_words
            self.device = device
            self.existing_srt = existing_srt
            
            # LLM é…ç½®
            self.llm_provider = llm_provider
            self.llm_api_key = llm_api_key
            self.llm_model = llm_model
            self.llm_base_url = llm_base_url
            
            suffix = '_llm_resplit.srt' if existing_srt else '_llm_smart.srt'
            self.result_file = RESULT_DIR + "/" + Path(video_file).stem + suffix

        def post(self, type='logs', text=""):
            self.uito.emit(json.dumps({"type": type, "text": text}))
        
        def get_file_hash(self, filepath):
            """è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
            import hashlib
            
            hash_obj = hashlib.sha256()
            try:
                with open(filepath, 'rb') as f:
                    # åˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜
                    for chunk in iter(lambda: f.read(8192), b''):
                        hash_obj.update(chunk)
                return hash_obj.hexdigest()
            except Exception as e:
                self.post(type='logs', text=f'âš ï¸ è®¡ç®—å“ˆå¸Œå€¼å¤±è´¥: {str(e)}')
                return None
        
        def get_cache_key(self, video_file, srt_file=None):
            """ç”Ÿæˆç¼“å­˜é”®"""
            video_hash = self.get_file_hash(video_file)
            if not video_hash:
                return None
            
            if srt_file:
                srt_hash = self.get_file_hash(srt_file)
                if not srt_hash:
                    return None
                return f"{video_hash}_{srt_hash}"
            
            return video_hash
        
        def save_cache(self, cache_key, all_words, language):
            """ä¿å­˜ç¼“å­˜"""
            import pickle
            
            if not cache_key:
                return
            
            cache_file = CACHE_DIR / f"{cache_key}.pkl"
            try:
                cache_data = {
                    'all_words': all_words,
                    'language': language,
                    'timestamp': __import__('time').time()
                }
                with open(cache_file, 'wb') as f:
                    pickle.dump(cache_data, f)
                self.post(type='logs', text=f'ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {cache_file.name}')
            except Exception as e:
                self.post(type='logs', text=f'âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}')
        
        def load_cache(self, cache_key):
            """åŠ è½½ç¼“å­˜"""
            import pickle
            
            if not cache_key:
                return None
            
            cache_file = CACHE_DIR / f"{cache_key}.pkl"
            if not cache_file.exists():
                return None
            
            try:
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                return cache_data
            except Exception as e:
                self.post(type='logs', text=f'âš ï¸ è¯»å–ç¼“å­˜å¤±è´¥: {str(e)}')
                return None

        def run(self):
            try:
                if self.existing_srt:
                    self.post(type='logs', text='ğŸ¤– æ¨¡å¼: LLMæ™ºèƒ½é‡æ–°åˆ†å‰²ç°æœ‰å­—å¹•')
                    self.process_with_existing_srt()
                else:
                    self.post(type='logs', text='ğŸ¤– æ¨¡å¼: LLMæ™ºèƒ½ç”Ÿæˆæ–°å­—å¹•')
                    self.process_new_transcription()
                
            except Exception as e:
                import traceback
                self.post(type='error', text=str(e) + "\n" + traceback.format_exc())
        
        def process_new_transcription(self):
            """ä»è§†é¢‘ç”Ÿæˆæ–°å­—å¹• + LLMä¼˜åŒ–"""
            # æ£€æŸ¥ç¼“å­˜
            self.post(type='logs', text='ğŸ” æ£€æŸ¥ç¼“å­˜...')
            cache_key = self.get_cache_key(self.video_file)
            cached_data = self.load_cache(cache_key)
            
            if cached_data:
                self.post(type='logs', text='âœ… æ‰¾åˆ°ç¼“å­˜ï¼ç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®')
                all_words = cached_data['all_words']
                detected_language = cached_data['language']
                self.post(type='logs', text=f'ğŸ“Š ä»ç¼“å­˜åŠ è½½: {len(all_words)} ä¸ªè¯')
                self.post(type='logs', text=f'ğŸŒ æ£€æµ‹è¯­è¨€: {detected_language}')
            else:
                self.post(type='logs', text='âŒ æœªæ‰¾åˆ°ç¼“å­˜ï¼Œå¼€å§‹ Whisper å¤„ç†...')
                self.post(type='logs', text='ğŸ”§ åŠ è½½ Faster-Whisper æ¨¡å‹...')
                
                try:
                    from faster_whisper import WhisperModel
                except ImportError:
                    self.post(type='error', text='æœªå®‰è£… faster-whisper\nè¯·è¿è¡Œ: pip install faster-whisper')
                    return
                
                self.post(type='logs', text=f'ğŸ“¥ æ¨¡å‹: {self.model_size}')
                
                # è®¾å¤‡ä¿¡æ¯
                device_name = {
                    'cpu': 'CPU',
                    'cuda': 'CUDA (NVIDIA GPU)',
                    'mps': 'MPS (Apple Silicon GPU)'
                }.get(self.device, self.device.upper())
                self.post(type='logs', text=f'âš™ï¸  è®¾å¤‡: {device_name}')
                
                # æ ¹æ®è®¾å¤‡é€‰æ‹©è®¡ç®—ç±»å‹
                if self.device == 'cuda':
                    compute_type = "float16"
                elif self.device == 'mps':
                    compute_type = "float16"
                else:
                    compute_type = "int8"
                
                # åŠ è½½æ¨¡å‹
                try:
                    model = WhisperModel(
                        self.model_size,
                        device=self.device,
                        compute_type=compute_type,
                        download_root=config.ROOT_DIR + "/models"
                    )
                except ValueError as e:
                    if 'unsupported device' in str(e).lower() and self.device == 'mps':
                        self.post(type='logs', text='âš ï¸  faster-whisper æš‚ä¸æ”¯æŒ MPS')
                        self.post(type='logs', text='ğŸ“¥ å›é€€åˆ° CPU æ¨¡å¼...')
                        self.device = 'cpu'
                        compute_type = 'int8'
                        model = WhisperModel(
                            self.model_size,
                            device='cpu',
                            compute_type='int8',
                            download_root=config.ROOT_DIR + "/models"
                        )
                    else:
                        raise
                
                self.post(type='logs', text=f'ğŸ¤ å¼€å§‹è¯†åˆ«è¯­éŸ³...')
                self.post(type='logs', text='â³ æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...')
                
                # è½¬å½•éŸ³é¢‘
                import time
                start_time = time.time()
                segments, info = model.transcribe(
                    self.video_file,
                    language=self.language if self.language != 'auto' else None,
                    word_timestamps=True,
                    beam_size=5,
                    vad_filter=True,
                    vad_parameters=dict(
                        threshold=0.5,
                        min_speech_duration_ms=250,
                        max_speech_duration_s=float('inf'),
                        min_silence_duration_ms=2000,
                        speech_pad_ms=400
                    )
                )
                transcribe_time = time.time() - start_time
                
                self.post(type='logs', text=f'âœ… è¯†åˆ«å®Œæˆï¼æ£€æµ‹è¯­è¨€: {info.language} (è€—æ—¶: {transcribe_time:.1f}ç§’)')
                self.post(type='logs', text='ğŸ“Š æ”¶é›†è¯çº§æ—¶é—´æˆ³...')
                
                # æ”¶é›†æ‰€æœ‰è¯
                all_words = []
                segment_count = 0
                for segment in segments:
                    segment_count += 1
                    if segment_count % 10 == 0:
                        self.post(type='logs', text=f'   å¤„ç†ç‰‡æ®µ: {segment_count}...')
                    
                    if hasattr(segment, 'words') and segment.words:
                        for word in segment.words:
                            all_words.append({
                                'word': word.word,
                                'start': word.start,
                                'end': word.end
                            })
                
                if not all_words:
                    self.post(type='error', text='æœªæ£€æµ‹åˆ°ä»»ä½•è¯­éŸ³å†…å®¹')
                    return
                
                self.post(type='logs', text=f'âœ… æ”¶é›†å®Œæˆï¼å…± {len(all_words)} ä¸ªè¯')
                
                # ä¿å­˜ç¼“å­˜
                detected_language = info.language
                self.save_cache(cache_key, all_words, detected_language)
            
            # ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½æ–­å¥
            self.post(type='logs', text='ğŸ¤– ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½æ–­å¥ä¼˜åŒ–...')
            subtitles = self.llm_smart_split(all_words, detected_language)
            
            if not subtitles:
                self.post(type='error', text='LLM æ–­å¥å¤±è´¥')
                return
            
            self.post(type='logs', text=f'âœ… ç”Ÿæˆ {len(subtitles)} æ¡å­—å¹•')
            
            # ä¿å­˜
            self.save_srt(subtitles)
            
            self.post(type='logs', text='ğŸ’¾ ä¿å­˜å®Œæˆ')
            self.post(type='ok', text=self.result_file)
        
        def process_with_existing_srt(self):
            """ä½¿ç”¨ç°æœ‰å­—å¹• + LLMé‡æ–°åˆ†å‰²"""
            import time
            import re
            
            self.post(type='logs', text=f'ğŸ“– è¯»å–ç°æœ‰å­—å¹•: {Path(self.existing_srt).name}')
            
            # è¯»å–ç°æœ‰å­—å¹•
            original_subtitles = self.parse_srt(self.existing_srt)
            if not original_subtitles:
                self.post(type='error', text='æ— æ³•è§£æå­—å¹•æ–‡ä»¶')
                return
            
            self.post(type='logs', text=f'âœ… è¯»å–åˆ° {len(original_subtitles)} æ¡åŸå§‹å­—å¹•')
            
            # æå–å®Œæ•´æ–‡æœ¬
            original_text = ' '.join([sub['text'] for sub in original_subtitles])
            self.post(type='logs', text=f'ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦: {len(original_text)} å­—ç¬¦')
            
            # æ£€æŸ¥ç¼“å­˜ï¼ˆåŒ…æ‹¬è§†é¢‘å’Œå­—å¹•æ–‡ä»¶ï¼‰
            self.post(type='logs', text='ğŸ” æ£€æŸ¥ç¼“å­˜...')
            cache_key = self.get_cache_key(self.video_file, self.existing_srt)
            cached_data = self.load_cache(cache_key)
            
            if cached_data:
                self.post(type='logs', text='âœ… æ‰¾åˆ°ç¼“å­˜ï¼ç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®')
                all_words = cached_data['all_words']
                detected_language = cached_data['language']
                self.post(type='logs', text=f'ğŸ“Š ä»ç¼“å­˜åŠ è½½: {len(all_words)} ä¸ªè¯')
                self.post(type='logs', text=f'ğŸŒ æ£€æµ‹è¯­è¨€: {detected_language}')
            else:
                self.post(type='logs', text='âŒ æœªæ‰¾åˆ°ç¼“å­˜ï¼Œå¼€å§‹ Whisper å¤„ç†...')
                
                # ä½¿ç”¨ Whisper è·å–è¯çº§æ—¶é—´æˆ³
                self.post(type='logs', text='ğŸ”§ åŠ è½½ Faster-Whisper æ¨¡å‹...')
                
                try:
                    from faster_whisper import WhisperModel
                except ImportError:
                    self.post(type='error', text='æœªå®‰è£… faster-whisper\nè¯·è¿è¡Œ: pip install faster-whisper')
                    return
                
                self.post(type='logs', text=f'ğŸ“¥ æ¨¡å‹: {self.model_size}')
                
                # è®¾å¤‡ä¿¡æ¯
                device_name = {
                    'cpu': 'CPU',
                    'cuda': 'CUDA (NVIDIA GPU)',
                    'mps': 'MPS (Apple Silicon GPU)'
                }.get(self.device, self.device.upper())
                self.post(type='logs', text=f'âš™ï¸  è®¾å¤‡: {device_name}')
                
                # æ ¹æ®è®¾å¤‡é€‰æ‹©è®¡ç®—ç±»å‹
                if self.device == 'cuda':
                    compute_type = "float16"
                elif self.device == 'mps':
                    compute_type = "float16"
                else:
                    compute_type = "int8"
                
                # åŠ è½½æ¨¡å‹
                try:
                    model = WhisperModel(
                        self.model_size,
                        device=self.device,
                        compute_type=compute_type,
                        download_root=config.ROOT_DIR + "/models"
                    )
                except ValueError as e:
                    if 'unsupported device' in str(e).lower() and self.device == 'mps':
                        self.post(type='logs', text='âš ï¸  faster-whisper æš‚ä¸æ”¯æŒ MPS')
                        self.post(type='logs', text='ğŸ“¥ å›é€€åˆ° CPU æ¨¡å¼...')
                        self.device = 'cpu'
                        compute_type = 'int8'
                        model = WhisperModel(
                            self.model_size,
                            device='cpu',
                            compute_type='int8',
                            download_root=config.ROOT_DIR + "/models"
                        )
                    else:
                        raise
                
                self.post(type='logs', text=f'ğŸ¤ å¼€å§‹è¯†åˆ«è¯­éŸ³ï¼ˆè·å–è¯çº§æ—¶é—´æˆ³ï¼‰...')
                
                # è½¬å½•éŸ³é¢‘
                start_time = time.time()
                segments, info = model.transcribe(
                    self.video_file,
                    language=self.language if self.language != 'auto' else None,
                    word_timestamps=True,
                    beam_size=5,
                    vad_filter=True,
                    vad_parameters=dict(
                        threshold=0.5,
                        min_speech_duration_ms=250,
                        max_speech_duration_s=float('inf'),
                        min_silence_duration_ms=2000,
                        speech_pad_ms=400
                    )
                )
                transcribe_time = time.time() - start_time
                
                self.post(type='logs', text=f'âœ… è¯†åˆ«å®Œæˆï¼æ£€æµ‹è¯­è¨€: {info.language} (è€—æ—¶: {transcribe_time:.1f}ç§’)')
                self.post(type='logs', text='ğŸ“Š æ”¶é›†è¯çº§æ—¶é—´æˆ³...')
                
                # æ”¶é›†æ‰€æœ‰è¯
                all_words = []
                segment_count = 0
                word_count = 0
                for segment in segments:
                    segment_count += 1
                    if segment_count % 10 == 0:
                        self.post(type='logs', text=f'   å¤„ç†ç‰‡æ®µ: {segment_count}... (å·²æ”¶é›† {word_count} ä¸ªè¯)')
                    
                    if hasattr(segment, 'words') and segment.words:
                        for word in segment.words:
                            all_words.append({
                                'word': word.word,
                                'start': word.start,
                                'end': word.end
                            })
                            word_count += 1
                
                if not all_words:
                    self.post(type='error', text='æœªæ£€æµ‹åˆ°ä»»ä½•è¯­éŸ³å†…å®¹')
                    return
                
                self.post(type='logs', text=f'âœ… æ”¶é›†å®Œæˆï¼å…±å¤„ç† {segment_count} ä¸ªç‰‡æ®µï¼Œ{len(all_words)} ä¸ªè¯')
                
                # ä¿å­˜ç¼“å­˜
                detected_language = info.language
                self.save_cache(cache_key, all_words, detected_language)
            
            # ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½æ–­å¥ï¼ˆä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼‰
            self.post(type='logs', text='ğŸ¤– ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½æ–­å¥ä¼˜åŒ–...')
            subtitles = self.llm_smart_split(all_words, detected_language, original_text=original_text)
            
            if not subtitles:
                self.post(type='error', text='LLM æ–­å¥å¤±è´¥')
                return
            
            self.post(type='logs', text=f'ğŸ“Š åŸå§‹å­—å¹•: {len(original_subtitles)} æ¡ â†’ æ–°å­—å¹•: {len(subtitles)} æ¡')
            
            # ä¿å­˜
            self.save_srt(subtitles)
            
            self.post(type='logs', text='ğŸ’¾ ä¿å­˜å®Œæˆ')
            self.post(type='ok', text=self.result_file)
        
        def llm_smart_split(self, words, detected_language, original_text=None):
            """ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½æ–­å¥"""
            import time
            
            if not words:
                return []
            
            # æ„å»ºè¯åˆ—è¡¨çš„æ–‡æœ¬è¡¨ç¤º
            words_with_index = []
            for i, w in enumerate(words):
                words_with_index.append(f"[{i}]{w['word']}")
            
            words_text = ''.join(words_with_index)
            
            # å¦‚æœæœ‰åŸå§‹æ–‡æœ¬ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼›å¦åˆ™ä½¿ç”¨è¯†åˆ«çš„æ–‡æœ¬
            reference_text = original_text if original_text else ''.join([w['word'] for w in words])
            
            # æ„å»º prompt
            prompt = self._build_llm_prompt(reference_text, len(words), detected_language)
            
            self.post(type='logs', text=f'   LLMæä¾›å•†: {self.llm_provider}')
            self.post(type='logs', text=f'   LLMæ¨¡å‹: {self.llm_model}')
            self.post(type='logs', text=f'   å¤„ç†æ–‡æœ¬: {len(words)} è¯')
            self.post(type='logs', text='   â³ æ­£åœ¨è°ƒç”¨ LLM APIï¼Œè¯·ç¨å€™...')
            
            # è°ƒç”¨ LLMï¼ˆæ”¯æŒæµå¼ä¼ è¾“ï¼‰
            start_time = time.time()
            try:
                self.post(type='logs', text='   ğŸ“¡ LLM å“åº”æµ:')
                response = self._call_llm_stream(prompt, words_text)
                llm_time = time.time() - start_time
                self.post(type='logs', text=f'\n   âœ… LLMå“åº”å®Œæˆ (è€—æ—¶: {llm_time:.1f}ç§’)')
            except Exception as e:
                self.post(type='logs', text=f'   âš ï¸  LLMè°ƒç”¨å¤±è´¥: {str(e)}')
                self.post(type='logs', text='   å›é€€åˆ°è§„åˆ™å¼•æ“æ–­å¥')
                return self.fallback_split(words)
            
            # è§£æ LLM è¿”å›çš„æ–­å¥ç»“æœ
            self.post(type='logs', text='   ğŸ“‹ è§£æ LLM è¿”å›ç»“æœ...')
            subtitles = self._parse_llm_response(response, words)
            
            if not subtitles:
                self.post(type='logs', text='   âš ï¸  LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œå›é€€åˆ°è§„åˆ™å¼•æ“')
                return self.fallback_split(words)
            
            self.post(type='logs', text=f'   âœ… è§£æå®Œæˆï¼Œç”Ÿæˆ {len(subtitles)} æ¡å­—å¹•')
            
            # éªŒè¯å’Œè°ƒæ•´æ—¶é—´æˆ³
            self.post(type='logs', text='   ğŸ”§ éªŒè¯å’Œè°ƒæ•´æ—¶é—´æˆ³...')
            subtitles = self._validate_and_adjust_timestamps(subtitles)
            
            self.post(type='logs', text='   âœ… æ—¶é—´æˆ³è°ƒæ•´å®Œæˆ')
            
            return subtitles
        
        def _build_llm_prompt(self, text, word_count, language):
            """æ„å»º LLM prompt"""
            
            lang_name = {
                'en': 'English',
                'zh': 'Chinese',
                'ja': 'Japanese',
                'ko': 'Korean',
                'es': 'Spanish',
                'fr': 'French',
                'de': 'German',
                'ru': 'Russian'
            }.get(language, 'English')
            
            prompt = f"""You are an expert subtitle editor. Your task is to split the following {lang_name} text into natural, readable subtitle segments.

TEXT TO SPLIT:
{text}

REQUIREMENTS:
1. Each subtitle should be 3-6 seconds when spoken (approximately {int(self.max_words * 0.7)}-{self.max_words} words)
2. Split at natural phrase boundaries (not in the middle of phrases)
3. Maintain semantic completeness (don't split incomplete thoughts)
4. Consider reading speed and viewer comprehension
5. Prioritize natural pauses in speech
6. Keep related concepts together (e.g., "a beautiful day" should not be split)

The text has {word_count} words total. Please split it into approximately {max(2, word_count // self.max_words)} segments.

OUTPUT FORMAT:
Return ONLY a JSON array of subtitle segments. Each segment should have:
- "text": the subtitle text
- "word_count": approximate number of words

Example output:
[
  {{"text": "Bringing people together these days is a feat.", "word_count": 8}},
  {{"text": "Thousands of people coming joyfully together", "word_count": 6}},
  {{"text": "to create a mile-long beautiful spectacle", "word_count": 7}}
]

DO NOT include explanations, only return the JSON array."""

            return prompt
        
        def _call_llm_stream(self, prompt, words_text):
            """è°ƒç”¨ LLM APIï¼ˆæµå¼ä¼ è¾“ï¼‰"""
            import requests
            import json
            
            if self.llm_provider == 'openai':
                return self._stream_openai(prompt)
            elif self.llm_provider == 'anthropic':
                return self._stream_anthropic(prompt)
            elif self.llm_provider == 'deepseek':
                return self._stream_deepseek(prompt)
            elif self.llm_provider == 'siliconflow':
                return self._stream_siliconflow(prompt)
            elif self.llm_provider == 'local':
                return self._stream_local_llm(prompt)
            else:
                raise ValueError(f'ä¸æ”¯æŒçš„ LLM æä¾›å•†: {self.llm_provider}')
        
        def _call_llm(self, prompt, words_text):
            """è°ƒç”¨ LLM APIï¼ˆéæµå¼ï¼Œå¤‡ç”¨ï¼‰"""
            import requests
            
            if self.llm_provider == 'openai':
                return self._call_openai(prompt)
            elif self.llm_provider == 'anthropic':
                return self._call_anthropic(prompt)
            elif self.llm_provider == 'deepseek':
                return self._call_deepseek(prompt)
            elif self.llm_provider == 'siliconflow':
                return self._call_siliconflow(prompt)
            elif self.llm_provider == 'local':
                return self._call_local_llm(prompt)
            else:
                raise ValueError(f'ä¸æ”¯æŒçš„ LLM æä¾›å•†: {self.llm_provider}')
        
        def _call_openai(self, prompt):
            """è°ƒç”¨ OpenAI API"""
            import requests
            
            url = self.llm_base_url if self.llm_base_url else 'https://api.openai.com/v1/chat/completions'
            
            headers = {
                'Authorization': f'Bearer {self.llm_api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.llm_model,
                'messages': [
                    {'role': 'system', 'content': 'You are an expert subtitle editor.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3,
                'response_format': {'type': 'json_object'} if 'gpt-4' in self.llm_model else None
            }
            
            # ç§»é™¤ None å€¼
            data = {k: v for k, v in data.items() if v is not None}
            
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result['choices'][0]['message']['content']
        
        def _call_anthropic(self, prompt):
            """è°ƒç”¨ Anthropic Claude API"""
            import requests
            
            url = 'https://api.anthropic.com/v1/messages'
            
            headers = {
                'x-api-key': self.llm_api_key,
                'anthropic-version': '2023-06-01',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.llm_model,
                'max_tokens': 4096,
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3
            }
            
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result['content'][0]['text']
        
        def _call_deepseek(self, prompt):
            """è°ƒç”¨ DeepSeek API"""
            import requests
            
            url = 'https://api.deepseek.com/v1/chat/completions'
            
            headers = {
                'Authorization': f'Bearer {self.llm_api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.llm_model,
                'messages': [
                    {'role': 'system', 'content': 'You are an expert subtitle editor.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3
            }
            
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result['choices'][0]['message']['content']
        
        def _call_siliconflow(self, prompt):
            """è°ƒç”¨ SiliconFlow API"""
            import requests
            
            url = 'https://api.siliconflow.cn/v1/chat/completions'
            
            headers = {
                'Authorization': f'Bearer {self.llm_api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.llm_model if self.llm_model else 'Qwen/Qwen2.5-7B-Instruct',
                'messages': [
                    {'role': 'system', 'content': 'You are an expert subtitle editor.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3
            }
            
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result['choices'][0]['message']['content']
        
        def _call_local_llm(self, prompt):
            """è°ƒç”¨æœ¬åœ° LLM (Ollama ç­‰)"""
            import requests
            
            url = self.llm_base_url if self.llm_base_url else 'http://localhost:11434/api/generate'
            
            data = {
                'model': self.llm_model,
                'prompt': prompt,
                'stream': False
            }
            
            response = requests.post(url, json=data, timeout=120)
            response.raise_for_status()
            
            result = response.json()
            return result.get('response', '')
        
        # ==================== æµå¼ä¼ è¾“æ–¹æ³• ====================
        
        def _stream_siliconflow(self, prompt):
            """è°ƒç”¨ SiliconFlow API (æµå¼ä¼ è¾“)"""
            import requests
            import json
            import time
            
            url = 'https://api.siliconflow.cn/v1/chat/completions'
            
            headers = {
                'Authorization': f'Bearer {self.llm_api_key}',
                'Content-Type': 'application/json',
                'Accept': 'text/event-stream'
            }
            
            data = {
                'model': self.llm_model if self.llm_model else 'Qwen/Qwen2.5-7B-Instruct',
                'messages': [
                    {'role': 'system', 'content': 'You are an expert subtitle editor.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3,
                'stream': True  # å¯ç”¨æµå¼ä¼ è¾“
            }
            
            # æ—¥å¿—ï¼šå‡†å¤‡å‘é€è¯·æ±‚
            self.post(type='logs', text=f'   ğŸŒ è¿æ¥åˆ°: {url}')
            self.post(type='logs', text=f'   ğŸ“¦ æ¨¡å‹: {data["model"]}')
            self.post(type='logs', text=f'   ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦')
            
            try:
                # æ—¥å¿—ï¼šå¼€å§‹å‘é€è¯·æ±‚
                self.post(type='logs', text=f'   ğŸ”„ æ­£åœ¨å‘é€ HTTP è¯·æ±‚...')
                request_start = time.time()
                
                response = requests.post(url, headers=headers, json=data, stream=True, timeout=120)
                
                # æ—¥å¿—ï¼šæ”¶åˆ°å“åº”
                request_time = time.time() - request_start
                self.post(type='logs', text=f'   âœ… æ”¶åˆ°å“åº”ï¼(è€—æ—¶: {request_time:.2f}ç§’)')
                self.post(type='logs', text=f'   ğŸ“Š HTTP çŠ¶æ€ç : {response.status_code}')
                self.post(type='logs', text=f'   ğŸ“‹ å“åº”å¤´: {dict(response.headers)}')
                
                response.raise_for_status()
                
                # æ—¥å¿—ï¼šå¼€å§‹è¯»å–æµ
                self.post(type='logs', text=f'   ğŸ”„ å¼€å§‹è¯»å–æµå¼æ•°æ®...')
                
            except requests.exceptions.Timeout:
                self.post(type='logs', text=f'   âŒ è¯·æ±‚è¶…æ—¶ï¼(120ç§’)')
                raise
            except requests.exceptions.ConnectionError as e:
                self.post(type='logs', text=f'   âŒ è¿æ¥é”™è¯¯: {str(e)}')
                raise
            except requests.exceptions.HTTPError as e:
                self.post(type='logs', text=f'   âŒ HTTP é”™è¯¯: {e.response.status_code}')
                try:
                    error_detail = e.response.json()
                    self.post(type='logs', text=f'   ğŸ“‹ é”™è¯¯è¯¦æƒ…: {error_detail}')
                except:
                    self.post(type='logs', text=f'   ğŸ“‹ é”™è¯¯å†…å®¹: {e.response.text[:500]}')
                raise
            except Exception as e:
                self.post(type='logs', text=f'   âŒ æœªçŸ¥é”™è¯¯: {str(e)}')
                raise
            
            full_content = []
            buffer = ""
            line_count = 0
            chunk_count = 0
            content_count = 0
            last_log_time = time.time()
            first_content_received = False
            
            try:
                for line in response.iter_lines():
                    line_count += 1
                    
                    if not line:
                        continue
                    
                    line = line.decode('utf-8')
                    
                    # ç¬¬ä¸€è¡Œæ•°æ®æ—¶è¾“å‡ºæ—¥å¿—
                    if line_count == 1:
                        self.post(type='logs', text=f'   ğŸ“¥ æ”¶åˆ°ç¬¬ä¸€è¡Œæ•°æ®ï¼Œå¼€å§‹æµå¼è¾“å‡º...')
                    
                    if line.startswith('data: '):
                        data_str = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                        
                        if data_str == '[DONE]':
                            self.post(type='logs', text=f'\n   ğŸ æµå¼ä¼ è¾“å®Œæˆï¼')
                            break
                        
                        try:
                            chunk = json.loads(data_str)
                            chunk_count += 1
                            
                            if 'choices' in chunk and len(chunk['choices']) > 0:
                                delta = chunk['choices'][0].get('delta', {})
                                content = delta.get('content', '')
                                if content:
                                    full_content.append(content)
                                    buffer += content
                                    content_count += len(content)
                                    
                                    # ç¬¬ä¸€æ¬¡æ”¶åˆ°å†…å®¹æ—¶æç¤º
                                    if not first_content_received:
                                        self.post(type='logs', text=f'   âœ¨ LLM å¼€å§‹ç”Ÿæˆå†…å®¹ï¼š')
                                        first_content_received = True
                                    
                                    # å®æ—¶æ˜¾ç¤ºæµå¼å†…å®¹ï¼ˆå¹²å‡€ï¼Œä¸å¸¦å¿ƒè·³ä¿¡æ¯ï¼‰
                                    self.post(type='stream', text=content)
                            elif 'error' in chunk:
                                self.post(type='logs', text=f'\n   âŒ API è¿”å›é”™è¯¯: {chunk["error"]}')
                                break
                        except json.JSONDecodeError as e:
                            # åªè®°å½•ç¬¬ä¸€ä¸ªè§£æé”™è¯¯
                            if chunk_count < 3:
                                self.post(type='logs', text=f'   âš ï¸  JSON è§£æå¤±è´¥ï¼Œç»§ç»­å°è¯•...')
                            continue
            
            except Exception as e:
                self.post(type='logs', text=f'\n   âš ï¸  æµå¼ä¼ è¾“å¼‚å¸¸: {str(e)}')
                import traceback
                self.post(type='logs', text=f'   ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}')
            
            # æ—¥å¿—ï¼šæ€»ç»“
            final_content = ''.join(full_content)
            self.post(type='logs', text=f'   ğŸ“Š æ¥æ”¶ç»Ÿè®¡: {chunk_count} ä¸ªæ•°æ®å—, {len(full_content)} ä¸ªå†…å®¹ç‰‡æ®µ')
            self.post(type='logs', text=f'   ğŸ“ æ€»å†…å®¹é•¿åº¦: {len(final_content)} å­—ç¬¦')
            
            return final_content
        
        def _stream_openai(self, prompt):
            """è°ƒç”¨ OpenAI API (æµå¼ä¼ è¾“)"""
            import requests
            import json
            
            url = self.llm_base_url if self.llm_base_url else 'https://api.openai.com/v1/chat/completions'
            
            headers = {
                'Authorization': f'Bearer {self.llm_api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.llm_model,
                'messages': [
                    {'role': 'system', 'content': 'You are an expert subtitle editor.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3,
                'stream': True
            }
            
            response = requests.post(url, headers=headers, json=data, stream=True, timeout=120)
            response.raise_for_status()
            
            full_content = []
            
            for line in response.iter_lines():
                if not line:
                    continue
                
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data_str = line[6:]
                    
                    if data_str == '[DONE]':
                        break
                    
                    try:
                        chunk = json.loads(data_str)
                        if 'choices' in chunk and len(chunk['choices']) > 0:
                            delta = chunk['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                full_content.append(content)
                                self.post(type='stream', text=content)
                    except json.JSONDecodeError:
                        continue
            
            return ''.join(full_content)
        
        def _stream_deepseek(self, prompt):
            """è°ƒç”¨ DeepSeek API (æµå¼ä¼ è¾“)"""
            import requests
            import json
            
            url = 'https://api.deepseek.com/v1/chat/completions'
            
            headers = {
                'Authorization': f'Bearer {self.llm_api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.llm_model,
                'messages': [
                    {'role': 'system', 'content': 'You are an expert subtitle editor.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3,
                'stream': True
            }
            
            response = requests.post(url, headers=headers, json=data, stream=True, timeout=120)
            response.raise_for_status()
            
            full_content = []
            
            for line in response.iter_lines():
                if not line:
                    continue
                
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data_str = line[6:]
                    
                    if data_str == '[DONE]':
                        break
                    
                    try:
                        chunk = json.loads(data_str)
                        if 'choices' in chunk and len(chunk['choices']) > 0:
                            delta = chunk['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                full_content.append(content)
                                self.post(type='stream', text=content)
                    except json.JSONDecodeError:
                        continue
            
            return ''.join(full_content)
        
        def _stream_anthropic(self, prompt):
            """è°ƒç”¨ Anthropic Claude API (æµå¼ä¼ è¾“)"""
            import requests
            import json
            
            url = 'https://api.anthropic.com/v1/messages'
            
            headers = {
                'x-api-key': self.llm_api_key,
                'anthropic-version': '2023-06-01',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': self.llm_model,
                'max_tokens': 4096,
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3,
                'stream': True
            }
            
            response = requests.post(url, headers=headers, json=data, stream=True, timeout=120)
            response.raise_for_status()
            
            full_content = []
            
            for line in response.iter_lines():
                if not line:
                    continue
                
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data_str = line[6:]
                    
                    try:
                        chunk = json.loads(data_str)
                        if chunk.get('type') == 'content_block_delta':
                            delta = chunk.get('delta', {})
                            content = delta.get('text', '')
                            if content:
                                full_content.append(content)
                                self.post(type='stream', text=content)
                    except json.JSONDecodeError:
                        continue
            
            return ''.join(full_content)
        
        def _stream_local_llm(self, prompt):
            """è°ƒç”¨æœ¬åœ° LLM (Ollama ç­‰ï¼Œæµå¼ä¼ è¾“)"""
            import requests
            import json
            
            url = self.llm_base_url if self.llm_base_url else 'http://localhost:11434/api/generate'
            
            data = {
                'model': self.llm_model,
                'prompt': prompt,
                'stream': True
            }
            
            response = requests.post(url, json=data, stream=True, timeout=120)
            response.raise_for_status()
            
            full_content = []
            
            for line in response.iter_lines():
                if not line:
                    continue
                
                try:
                    chunk = json.loads(line)
                    content = chunk.get('response', '')
                    if content:
                        full_content.append(content)
                        self.post(type='stream', text=content)
                    
                    if chunk.get('done', False):
                        break
                except json.JSONDecodeError:
                    continue
            
            return ''.join(full_content)
        
        # ==================== è§£æå’ŒéªŒè¯æ–¹æ³• ====================
        
        def _parse_llm_response(self, response, words):
            """è§£æ LLM è¿”å›çš„ç»“æœï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒç¼ºå¤±è¯çš„æ—¶é—´æˆ³æ’å€¼ï¼‰"""
            import json
            import re
            
            # å°è¯•æå– JSON
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if not json_match:
                return []
            
            try:
                segments = json.loads(json_match.group(0))
            except json.JSONDecodeError:
                return []
            
            if not isinstance(segments, list):
                return []
            
            # å°† LLM è¿”å›çš„æ–‡æœ¬æ®µåŒ¹é…åˆ°è¯çº§æ—¶é—´æˆ³
            subtitles = []
            word_idx = 0
            
            for segment in segments:
                if not isinstance(segment, dict) or 'text' not in segment:
                    continue
                
                segment_text = segment['text'].strip()
                if not segment_text:
                    continue
                
                # ä½¿ç”¨å¢å¼ºçš„åŒ¹é…ç­–ç•¥
                match_result = self._match_text_to_words(segment_text, words, word_idx)
                
                if match_result:
                    subtitle = {
                        'start': match_result['start'],
                        'end': match_result['end'],
                        'text': segment_text
                    }
                    subtitles.append(subtitle)
                    word_idx = match_result['next_idx']
            
            return subtitles
        
        def _match_text_to_words(self, text, words, start_idx):
            """
            å¢å¼ºçš„æ–‡æœ¬åˆ°å•è¯æ—¶é—´æˆ³åŒ¹é…ç®—æ³•
            
            æ”¯æŒï¼š
            1. ç¼ºå¤±è¯çš„å¤„ç†ï¼ˆWhisper æœªè¯†åˆ«çš„è¯ï¼‰
            2. æ—¶é—´æˆ³æ’å€¼ä¼°ç®—
            3. æ›´æ™ºèƒ½çš„åºåˆ—å¯¹é½
            """
            import re
            
            # æ¸…ç†å’Œåˆ†è¯
            text_clean = text.lower()
            for punct in [',', '.', '!', '?', ';', ':', '"', "'", '(', ')', '[', ']']:
                text_clean = text_clean.replace(punct, ' ')
            text_words = [w for w in text_clean.split() if w]
            
            if not text_words:
                return None
            
            # ä½¿ç”¨åŠ¨æ€è§„åˆ’è¿›è¡Œåºåˆ—å¯¹é½
            matched_indices = []
            text_idx = 0
            word_idx = start_idx
            max_lookahead = 15  # å¢åŠ å‰ç»èŒƒå›´
            
            while text_idx < len(text_words) and word_idx < len(words):
                text_word = text_words[text_idx]
                best_match = None
                best_score = 0
                best_offset = 0
                
                # åœ¨å½“å‰ä½ç½®é™„è¿‘æŸ¥æ‰¾æœ€ä½³åŒ¹é…
                for offset in range(min(max_lookahead, len(words) - word_idx)):
                    if word_idx + offset >= len(words):
                        break
                    
                    word_data = words[word_idx + offset]
                    word_text = word_data['word'].lower().strip()
                    
                    # æ¸…ç†å•è¯
                    for punct in [',', '.', '!', '?', ';', ':', '"', "'", '(', ')', '[', ']']:
                        word_text = word_text.replace(punct, '')
                    word_text = word_text.strip()
                    
                    if not word_text:
                        continue
                    
                    # è®¡ç®—åŒ¹é…åˆ†æ•°
                    score = self._calculate_match_score(text_word, word_text)
                    
                    # è€ƒè™‘ä½ç½®å› ç´ ï¼ˆè¶Šè¿‘è¶Šå¥½ï¼‰
                    score = score - (offset * 0.1)
                    
                    if score > best_score:
                        best_score = score
                        best_match = word_idx + offset
                        best_offset = offset
                
                # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼ˆé˜ˆå€¼ï¼š0.5ï¼‰
                if best_score > 0.5:
                    matched_indices.append(best_match)
                    word_idx = best_match + 1
                    text_idx += 1
                else:
                    # æœªæ‰¾åˆ°åŒ¹é…ï¼Œå¯èƒ½æ˜¯ Whisper ç¼ºå¤±çš„è¯
                    # è·³è¿‡è¿™ä¸ªæ–‡æœ¬è¯ï¼Œä½†ä¸ç§»åŠ¨ word_idx
                    text_idx += 1
            
            if not matched_indices:
                return None
            
            # è·å–åŒ¹é…åˆ°çš„å•è¯çš„æ—¶é—´æˆ³
            matched_words = [words[i] for i in matched_indices]
            
            # è®¡ç®—èµ·æ­¢æ—¶é—´ï¼ˆè€ƒè™‘ç¼ºå¤±è¯çš„æƒ…å†µï¼‰
            start_time = matched_words[0]['start']
            end_time = matched_words[-1]['end']
            
            # å¦‚æœåŒ¹é…ç‡å¤ªä½ï¼Œå°è¯•æ’å€¼ä¼°ç®—
            match_ratio = len(matched_indices) / len(text_words)
            if match_ratio < 0.5:
                # åŒ¹é…ç‡ä½äº50%ï¼Œå¯èƒ½æœ‰å¾ˆå¤šç¼ºå¤±è¯
                # ä½¿ç”¨æ›´ä¿å®ˆçš„æ—¶é—´èŒƒå›´ä¼°ç®—
                if len(matched_indices) >= 2:
                    # åŸºäºåŒ¹é…è¯çš„å¯†åº¦ä¼°ç®—æ€»æ—¶é•¿
                    avg_word_duration = (end_time - start_time) / len(matched_indices)
                    estimated_duration = avg_word_duration * len(text_words)
                    
                    # è°ƒæ•´ç»“æŸæ—¶é—´
                    end_time = start_time + estimated_duration
                else:
                    # åªæœ‰ä¸€ä¸ªåŒ¹é…è¯ï¼Œä½¿ç”¨é»˜è®¤ä¼°ç®—
                    avg_duration_per_word = 0.3  # å‡è®¾æ¯è¯0.3ç§’
                    end_time = start_time + (len(text_words) * avg_duration_per_word)
            
            return {
                'start': start_time,
                'end': end_time,
                'next_idx': word_idx,
                'match_ratio': match_ratio  # ç”¨äºè°ƒè¯•
            }
        
        def _calculate_match_score(self, text_word, whisper_word):
            """
            è®¡ç®—ä¸¤ä¸ªè¯çš„åŒ¹é…åˆ†æ•°
            
            è¿”å›å€¼ï¼š0.0 - 1.0
            """
            if not text_word or not whisper_word:
                return 0.0
            
            # å®Œå…¨åŒ¹é…
            if text_word == whisper_word:
                return 1.0
            
            # ä¸€ä¸ªåŒ…å«å¦ä¸€ä¸ª
            if text_word in whisper_word or whisper_word in text_word:
                shorter = min(len(text_word), len(whisper_word))
                longer = max(len(text_word), len(whisper_word))
                return shorter / longer * 0.9
            
            # ä½¿ç”¨ç¼–è¾‘è·ç¦»
            distance = self._levenshtein_distance(text_word, whisper_word)
            max_len = max(len(text_word), len(whisper_word))
            
            if max_len == 0:
                return 0.0
            
            similarity = 1.0 - (distance / max_len)
            
            # åªæœ‰ç›¸ä¼¼åº¦è¶³å¤Ÿé«˜æ‰è®¤ä¸ºæ˜¯åŒ¹é…
            return similarity if similarity > 0.6 else 0.0
        
        def _levenshtein_distance(self, s1, s2):
            """è®¡ç®—ç¼–è¾‘è·ç¦»"""
            if len(s1) < len(s2):
                return self._levenshtein_distance(s2, s1)
            
            if len(s2) == 0:
                return len(s1)
            
            previous_row = range(len(s2) + 1)
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    # j+1 instead of j since previous_row and current_row are one character longer than s2
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            
            return previous_row[-1]
        
        def _validate_and_adjust_timestamps(self, subtitles):
            """éªŒè¯å’Œè°ƒæ•´æ—¶é—´æˆ³"""
            if not subtitles:
                return []
            
            validated = []
            
            for i, sub in enumerate(subtitles):
                # ç¡®ä¿æ—¶é—´æˆ³åˆæ³•
                if sub['start'] >= sub['end']:
                    sub['end'] = sub['start'] + 1.0
                
                # ç¡®ä¿ä¸ä¸å‰ä¸€æ¡é‡å 
                if i > 0 and sub['start'] < validated[-1]['end']:
                    sub['start'] = validated[-1]['end'] + 0.01
                    if sub['start'] >= sub['end']:
                        sub['end'] = sub['start'] + 1.0
                
                # æ£€æŸ¥æŒç»­æ—¶é—´æ˜¯å¦åˆç†
                duration = sub['end'] - sub['start']
                if duration > self.max_duration * 2:
                    # å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­
                    sub['end'] = sub['start'] + self.max_duration * 1.5
                elif duration < 0.5:
                    # å¦‚æœå¤ªçŸ­ï¼Œå»¶é•¿
                    sub['end'] = sub['start'] + 0.5
                
                validated.append(sub)
            
            return validated
        
        def fallback_split(self, words):
            """å›é€€åˆ°è§„åˆ™å¼•æ“æ–­å¥ï¼ˆå½“LLMå¤±è´¥æ—¶ï¼‰"""
            self.post(type='logs', text='   ğŸ”„ ä½¿ç”¨è§„åˆ™å¼•æ“æ–­å¥...')
            
            # æ£€æŸ¥è¾“å…¥
            if not words or len(words) == 0:
                self.post(type='logs', text='   âš ï¸  è¯åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ–­å¥')
                return []
            
            try:
                # ä½¿ç”¨ç®€åŒ–çš„è§„åˆ™å¼•æ“
                subtitles = []
                current_words = []
                current_start = words[0]['start']
                
                sentence_ends = {'.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ'}
                
                for i, word in enumerate(words):
                    current_words.append(word)
                    duration = word['end'] - current_start
                    word_text = word['word'].strip()
                    
                    should_split = False
                    
                    # å¥å­ç»“æŸ
                    if word_text and word_text[-1] in sentence_ends:
                        should_split = True
                    # è¶…é™åˆ¶
                    elif duration >= self.max_duration or len(current_words) >= self.max_words:
                        should_split = True
                    
                    if should_split and current_words:
                        subtitle = {
                            'start': current_start,
                            'end': current_words[-1]['end'],
                            'text': ''.join([w['word'] for w in current_words]).strip(),
                        }
                        subtitles.append(subtitle)
                        current_words = []
                        if i + 1 < len(words):
                            current_start = words[i + 1]['start']
                
                # å¤„ç†å‰©ä½™çš„è¯
                if current_words:
                    subtitle = {
                        'start': current_start,
                        'end': current_words[-1]['end'],
                        'text': ''.join([w['word'] for w in current_words]).strip(),
                    }
                    subtitles.append(subtitle)
                
                self.post(type='logs', text=f'   âœ… è§„åˆ™å¼•æ“ç”Ÿæˆ {len(subtitles)} æ¡å­—å¹•')
                return subtitles
                
            except Exception as e:
                self.post(type='logs', text=f'   âŒ è§„åˆ™å¼•æ“å¤±è´¥: {str(e)}')
                import traceback
                self.post(type='logs', text=f'   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}')
                return []
        
        def parse_srt(self, srt_file):
            """è§£æ SRT æ–‡ä»¶"""
            import re
            
            try:
                with open(srt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
            except:
                try:
                    with open(srt_file, 'r', encoding='utf-8-sig') as f:
                        content = f.read()
                except:
                    return []
            
            pattern = r'(\d+)\s*\n(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})\s*\n((?:.*\n)*?)(?:\n|$)'
            matches = re.findall(pattern, content)
            
            subtitles = []
            for match in matches:
                start_time = self.parse_timestamp(match[1])
                end_time = self.parse_timestamp(match[2])
                text = match[3].strip()
                
                if text:
                    subtitles.append({
                        'start': start_time,
                        'end': end_time,
                        'text': text
                    })
            
            return subtitles
        
        def parse_timestamp(self, timestamp_str):
            """å°† SRT æ—¶é—´æˆ³è½¬æ¢ä¸ºç§’"""
            import re
            match = re.match(r'(\d{2}):(\d{2}):(\d{2}),(\d{3})', timestamp_str)
            if match:
                h, m, s, ms = map(int, match.groups())
                return h * 3600 + m * 60 + s + ms / 1000.0
            return 0.0
        
        def save_srt(self, subtitles):
            """ä¿å­˜ä¸ºSRTæ ¼å¼"""
            with open(self.result_file, 'w', encoding='utf-8') as f:
                for i, sub in enumerate(subtitles, 1):
                    f.write(f"{i}\n")
                    f.write(f"{self.format_timestamp(sub['start'])} --> {self.format_timestamp(sub['end'])}\n")
                    f.write(f"{sub['text']}\n")
                    f.write("\n")
        
        def format_timestamp(self, seconds):
            """è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = int(seconds % 60)
            milliseconds = int((seconds % 1) * 1000)
            return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"

    def feed(d):
        if winobj.has_done:
            return
        d = json.loads(d)
        if d['type'] == "error":
            winobj.has_done = True
            winobj.loglabel.setPlainText(d['text'])
            # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
            winobj.loglabel.verticalScrollBar().setValue(
                winobj.loglabel.verticalScrollBar().maximum()
            )
            tools.show_error(d['text'])
            winobj.startbtn.setText('å¼€å§‹ç”Ÿæˆ' if config.defaulelang == 'zh' else 'Start Generate')
            winobj.startbtn.setDisabled(False)
        elif d['type'] == 'logs':
            current_text = winobj.loglabel.toPlainText()
            winobj.loglabel.setPlainText(current_text + '\n' + d['text'])
            # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
            winobj.loglabel.verticalScrollBar().setValue(
                winobj.loglabel.verticalScrollBar().maximum()
            )
        elif d['type'] == 'stream':
            # æµå¼å†…å®¹ï¼šè¿½åŠ åˆ°å½“å‰è¡Œæœ«å°¾ï¼Œä¸æ¢è¡Œ
            current_text = winobj.loglabel.toPlainText()
            winobj.loglabel.setPlainText(current_text + d['text'])
            # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
            winobj.loglabel.verticalScrollBar().setValue(
                winobj.loglabel.verticalScrollBar().maximum()
            )
        else:
            winobj.has_done = True
            winobj.startbtn.setText('å¼€å§‹ç”Ÿæˆ' if config.defaulelang == 'zh' else 'Start Generate')
            winobj.startbtn.setDisabled(False)
            winobj.resultlabel.setText(d['text'])
            winobj.resultbtn.setDisabled(False)
            winobj.resultinput.setPlainText(Path(winobj.resultlabel.text()).read_text(encoding='utf-8'))
            winobj.loglabel.setPlainText(winobj.loglabel.toPlainText() + '\n\nâœ… ç”Ÿæˆå®Œæˆï¼')
            # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
            winobj.loglabel.verticalScrollBar().setValue(
                winobj.loglabel.verticalScrollBar().maximum()
            )

    def toggle_srt_input():
        """åˆ‡æ¢å­—å¹•æ–‡ä»¶è¾“å…¥æ¡†çš„æ˜¾ç¤º"""
        is_checked = winobj.use_existing_srt_checkbox.isChecked()
        winobj.srtbtn.setVisible(is_checked)
        winobj.srtinput.setVisible(is_checked)
        if not is_checked:
            winobj.srtinput.setText("æœªé€‰æ‹©å­—å¹•æ–‡ä»¶" if config.defaulelang == 'zh' else 'No subtitle file selected')
    
    def toggle_llm_settings():
        """åˆ‡æ¢ LLM è®¾ç½®çš„æ˜¾ç¤º"""
        is_checked = winobj.use_llm_checkbox.isChecked()
        # æ˜¾ç¤º/éšè— LLM é…ç½®
        winobj.llm_provider_combo.setVisible(is_checked)
        winobj.llm_provider_label.setVisible(is_checked)
        winobj.llm_api_key_input.setVisible(is_checked)
        winobj.llm_api_key_label.setVisible(is_checked)
        winobj.llm_model_combo.setVisible(is_checked)
        winobj.llm_model_label.setVisible(is_checked)
        winobj.llm_base_url_input.setVisible(is_checked)
        winobj.llm_base_url_label.setVisible(is_checked)
        winobj.llm_test_btn.setVisible(is_checked)
        
        # å‹¾é€‰ LLM æ—¶ï¼Œéšè—æœ€å¤§æŒç»­æ—¶é—´å’Œæœ€å¤§è¯æ•°ï¼ˆLLM ä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼‰
        # ä¸å‹¾é€‰æ—¶æ˜¾ç¤ºè¿™äº›å‚æ•°ï¼ˆè§„åˆ™å¼•æ“éœ€è¦ï¼‰
        winobj.duration_input.setVisible(not is_checked)
        winobj.duration_label.setVisible(not is_checked)
        winobj.words_input.setVisible(not is_checked)
        winobj.words_label.setVisible(not is_checked)
    
    def save_api_key_to_env():
        """ä¿å­˜ API Key åˆ° .env æ–‡ä»¶ï¼Œæ”¯æŒå¤šä¸ªæä¾›å•†"""
        import os
        api_key = winobj.llm_api_key_input.text().strip()
        if not api_key:
            return
        
        provider = winobj.llm_provider_combo.currentText().lower()
        env_file = os.path.join(config.ROOT_DIR, '.env')
        
        # æ ¹æ®æä¾›å•†ç¡®å®šç¯å¢ƒå˜é‡åç§°
        env_key_map = {
            'openai': 'OPENAI_API_KEY',
            'anthropic': 'ANTHROPIC_API_KEY',
            'gemini': 'GEMINI_API_KEY',
            'deepseek': 'DEEPSEEK_API_KEY',
            'siliconflow': 'SILICONFLOW_API_KEY',
            'local': 'LOCAL_LLM_API_KEY'
        }
        env_key_name = env_key_map.get(provider, 'SILICONFLOW_API_KEY')
        
        # è¯»å–ç°æœ‰çš„ .env æ–‡ä»¶å†…å®¹
        lines = []
        key_exists = False
        
        if os.path.exists(env_file):
            try:
                with open(env_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # æŸ¥æ‰¾å¹¶æ›´æ–°å¯¹åº”çš„ API Key
                for i, line in enumerate(lines):
                    if line.strip().startswith(f'{env_key_name}='):
                        lines[i] = f'{env_key_name}={api_key}\n'
                        key_exists = True
                        break
            except Exception as e:
                print(f"è¯»å– .env æ–‡ä»¶å¤±è´¥: {e}")
        
        # å¦‚æœ key ä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾
        if not key_exists:
            if lines and not lines[-1].endswith('\n'):
                lines.append('\n')
            lines.append(f'{env_key_name}={api_key}\n')
        
        # å†™å›æ–‡ä»¶
        try:
            with open(env_file, 'w', encoding='utf-8') as f:
                f.writelines(lines)
            print(f"API Key å·²ä¿å­˜åˆ° {env_file}")
        except Exception as e:
            print(f"ä¿å­˜ API Key å¤±è´¥: {e}")
    
    def save_llm_config():
        """ä¿å­˜ LLM é…ç½®åˆ° config.paramsï¼Œä¸å­—å¹•ç¿»è¯‘å…±äº«"""
        provider = winobj.llm_provider_combo.currentText().lower()
        model = winobj.llm_model_combo.currentText()
        base_url = winobj.llm_base_url_input.text().strip()
        
        # ä¿å­˜åˆ° config.params
        config.params['llm_provider'] = provider
        config.params['llm_model'] = model
        config.params['llm_base_url'] = base_url
        config.getset_params(config.params)
    
    def load_api_key_from_env():
        """ä» .env æ–‡ä»¶åŠ è½½ API Keyï¼Œæ ¹æ®å½“å‰é€‰æ‹©çš„æä¾›å•†"""
        import os
        provider = winobj.llm_provider_combo.currentText().lower()
        
        # æ ¹æ®æä¾›å•†ç¡®å®šç¯å¢ƒå˜é‡åç§°
        env_key_map = {
            'openai': 'OPENAI_API_KEY',
            'anthropic': 'ANTHROPIC_API_KEY',
            'gemini': 'GEMINI_API_KEY',
            'deepseek': 'DEEPSEEK_API_KEY',
            'siliconflow': 'SILICONFLOW_API_KEY',
            'local': 'LOCAL_LLM_API_KEY'
        }
        env_key_name = env_key_map.get(provider, 'SILICONFLOW_API_KEY')
        
        api_key = ""
        # é¦–å…ˆå°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
        api_key = os.environ.get(env_key_name, '')
        
        # å¦‚æœç¯å¢ƒå˜é‡æ²¡æœ‰ï¼Œå°è¯•ä» .env æ–‡ä»¶è¯»å–
        if not api_key:
            env_file = os.path.join(config.ROOT_DIR, '.env')
            if os.path.exists(env_file):
                try:
                    with open(env_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line and not line.startswith('#'):
                                if '=' in line:
                                    key, value = line.split('=', 1)
                                    key = key.strip()
                                    value = value.strip().strip('"').strip("'")
                                    if key == env_key_name:
                                        api_key = value
                                        break
                except Exception as e:
                    print(f"è¯»å– .env æ–‡ä»¶å¤±è´¥: {e}")
        
        # è®¾ç½®åˆ°è¾“å…¥æ¡†
        if api_key:
            winobj.llm_api_key_input.setText(api_key)
    
    def provider_changed_callback():
        """æä¾›å•†æ”¹å˜æ—¶çš„å›è°ƒ"""
        # åŠ è½½å¯¹åº”æä¾›å•†çš„ API Key
        load_api_key_from_env()
        # ä¿å­˜é…ç½®
        save_llm_config()
    
    class TestLLMThread(QThread):
        """å¼‚æ­¥æµ‹è¯•LLMè¿æ¥çš„çº¿ç¨‹"""
        finished = Signal(str, bool)  # ä¿¡å·ï¼š(æ¶ˆæ¯, æ˜¯å¦æˆåŠŸ)
        progress = Signal(str)  # è¿›åº¦ä¿¡å·
        
        def __init__(self, provider, api_key, model, base_url):
            super().__init__()
            self.provider = provider
            self.api_key = api_key
            self.model = model
            self.base_url = base_url
        
        def run(self):
            try:
                import requests
                
                # å‘é€è¿›åº¦æ›´æ–°
                self.progress.emit('â³ æ­£åœ¨æ„å»ºæµ‹è¯•è¯·æ±‚...' if config.defaulelang == 'zh' else 'â³ Building test request...')
                
                # æ„å»ºæµ‹è¯•è¯·æ±‚
                test_prompt = "è¯·å›å¤'OK'ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿æ¥æµ‹è¯•ã€‚" if config.defaulelang == 'zh' else "Reply 'OK', this is a connection test."
                
                if self.provider == 'openai':
                    url = self.base_url if self.base_url else 'https://api.openai.com/v1/chat/completions'
                    headers = {
                        'Authorization': f'Bearer {self.api_key}',
                        'Content-Type': 'application/json'
                    }
                    data = {
                        'model': self.model,
                        'messages': [{'role': 'user', 'content': test_prompt}],
                        'max_tokens': 10
                    }
                
                elif self.provider == 'anthropic':
                    url = 'https://api.anthropic.com/v1/messages'
                    headers = {
                        'x-api-key': self.api_key,
                        'anthropic-version': '2023-06-01',
                        'Content-Type': 'application/json'
                    }
                    data = {
                        'model': self.model,
                        'max_tokens': 10,
                        'messages': [{'role': 'user', 'content': test_prompt}]
                    }
                
                elif self.provider == 'deepseek':
                    url = 'https://api.deepseek.com/v1/chat/completions'
                    headers = {
                        'Authorization': f'Bearer {self.api_key}',
                        'Content-Type': 'application/json'
                    }
                    data = {
                        'model': self.model,
                        'messages': [{'role': 'user', 'content': test_prompt}],
                        'max_tokens': 10
                    }
                
                elif self.provider == 'siliconflow':
                    url = self.base_url if self.base_url else 'https://api.siliconflow.cn/v1/chat/completions'
                    headers = {
                        'Authorization': f'Bearer {self.api_key}',
                        'Content-Type': 'application/json'
                    }
                    data = {
                        'model': self.model,
                        'messages': [{'role': 'user', 'content': test_prompt}],
                        'max_tokens': 10
                    }
                
                elif self.provider == 'local':
                    url = self.base_url if self.base_url else 'http://localhost:11434/api/generate'
                    data = {
                        'model': self.model,
                        'prompt': test_prompt,
                        'stream': False
                    }
                    headers = {}
                
                else:
                    self.finished.emit(
                        f'ä¸æ”¯æŒçš„æä¾›å•†: {self.provider}' if config.defaulelang == 'zh' else f'Unsupported provider: {self.provider}',
                        False
                    )
                    return
                
                # å‘é€è¿›åº¦æ›´æ–°
                self.progress.emit('ğŸ“¡ æ­£åœ¨è¿æ¥æœåŠ¡å™¨...' if config.defaulelang == 'zh' else 'ğŸ“¡ Connecting to server...')
                
                # å‘é€æµ‹è¯•è¯·æ±‚
                response = requests.post(url, headers=headers, json=data, timeout=30)
                
                # å‘é€è¿›åº¦æ›´æ–°
                self.progress.emit('ğŸ“¥ æ­£åœ¨è§£æå“åº”...' if config.defaulelang == 'zh' else 'ğŸ“¥ Parsing response...')
                
                # æ£€æŸ¥å“åº”
                if response.status_code == 200:
                    result = response.json()
                    # éªŒè¯å“åº”æ ¼å¼
                    if self.provider in ['openai', 'deepseek', 'siliconflow']:
                        if 'choices' in result and len(result['choices']) > 0:
                            success_msg = f'âœ… è¿æ¥æˆåŠŸï¼æä¾›å•†: {self.provider} | æ¨¡å‹: {self.model} | å“åº”æ­£å¸¸' if config.defaulelang == 'zh' else f'âœ… Connection Successful! Provider: {self.provider} | Model: {self.model} | Response OK'
                            self.finished.emit(success_msg, True)
                        else:
                            self.finished.emit('å“åº”æ ¼å¼ä¸æ­£ç¡®' if config.defaulelang == 'zh' else 'Invalid response format', False)
                    
                    elif self.provider == 'anthropic':
                        if 'content' in result:
                            success_msg = f'âœ… è¿æ¥æˆåŠŸï¼æä¾›å•†: {self.provider} | æ¨¡å‹: {self.model} | å“åº”æ­£å¸¸' if config.defaulelang == 'zh' else f'âœ… Connection Successful! Provider: {self.provider} | Model: {self.model} | Response OK'
                            self.finished.emit(success_msg, True)
                        else:
                            self.finished.emit('å“åº”æ ¼å¼ä¸æ­£ç¡®' if config.defaulelang == 'zh' else 'Invalid response format', False)
                    
                    elif self.provider == 'local':
                        if 'response' in result:
                            success_msg = f'âœ… è¿æ¥æˆåŠŸï¼æä¾›å•†: {self.provider} | æ¨¡å‹: {self.model} | å“åº”æ­£å¸¸' if config.defaulelang == 'zh' else f'âœ… Connection Successful! Provider: {self.provider} | Model: {self.model} | Response OK'
                            self.finished.emit(success_msg, True)
                        else:
                            self.finished.emit('å“åº”æ ¼å¼ä¸æ­£ç¡®' if config.defaulelang == 'zh' else 'Invalid response format', False)
                else:
                    error_msg = f'âŒ è¿æ¥å¤±è´¥ï¼HTTP {response.status_code}: {response.text[:200]}' if config.defaulelang == 'zh' else f'âŒ Connection Failed! HTTP {response.status_code}: {response.text[:200]}'
                    self.finished.emit(error_msg, False)
            
            except requests.exceptions.Timeout:
                self.finished.emit(
                    'âŒ è¿æ¥è¶…æ—¶ï¼è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥' if config.defaulelang == 'zh' else 'âŒ Connection Timeout! Please check network connection',
                    False
                )
            
            except requests.exceptions.ConnectionError:
                self.finished.emit(
                    'âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼è¯·æ£€æŸ¥ç½‘ç»œæˆ– Base URL' if config.defaulelang == 'zh' else 'âŒ Cannot connect to server! Please check network or Base URL',
                    False
                )
            
            except Exception as e:
                error_msg = f'âŒ æµ‹è¯•å¤±è´¥ï¼{str(e)}' if config.defaulelang == 'zh' else f'âŒ Test Failed! {str(e)}'
                self.finished.emit(error_msg, False)
    
    def test_llm_connection():
        """æµ‹è¯• LLM è¿æ¥æ˜¯å¦æ­£å¸¸ï¼ˆå¼‚æ­¥ï¼‰"""
        # è·å–é…ç½®
        provider = winobj.llm_provider_combo.currentText().lower()
        api_key = winobj.llm_api_key_input.text()
        model = winobj.llm_model_combo.currentText()
        base_url = winobj.llm_base_url_input.text()
        
        # æ¸…ç©ºæ—¥å¿—æˆ–è·å–å½“å‰æ—¥å¿—å†…å®¹
        current_log = winobj.loglabel.toPlainText()
        if current_log in ["å¤„ç†æ—¥å¿—å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...", "Processing log will be displayed here..."]:
            winobj.loglabel.clear()
        
        # éªŒè¯å¿…å¡«é¡¹
        if not api_key and provider != 'local':
            msg = 'âŒ è¯·è¾“å…¥ API Key' if config.defaulelang == 'zh' else 'âŒ Please enter API Key'
            winobj.loglabel.appendPlainText(f'\n{msg}')
            # è‡ªåŠ¨æ»šåŠ¨
            winobj.loglabel.verticalScrollBar().setValue(
                winobj.loglabel.verticalScrollBar().maximum()
            )
            return
        
        if not model:
            msg = 'âŒ è¯·é€‰æ‹©æ¨¡å‹' if config.defaulelang == 'zh' else 'âŒ Please select model'
            winobj.loglabel.appendPlainText(f'\n{msg}')
            # è‡ªåŠ¨æ»šåŠ¨
            winobj.loglabel.verticalScrollBar().setValue(
                winobj.loglabel.verticalScrollBar().maximum()
            )
            return
        
        # ç¦ç”¨æŒ‰é’®ï¼Œæ˜¾ç¤ºæµ‹è¯•ä¸­
        winobj.llm_test_btn.setDisabled(True)
        winobj.llm_test_btn.setText('â³\næµ‹è¯•ä¸­' if config.defaulelang == 'zh' else 'â³\nTesting')
        
        # åœ¨æ—¥å¿—ä¸­æ˜¾ç¤ºå¼€å§‹æµ‹è¯•
        if config.defaulelang == 'zh':
            test_start_msg = f'\n{"="*50}\nğŸ” å¼€å§‹æµ‹è¯• LLM è¿æ¥...\n{"="*50}\nğŸ“Œ æä¾›å•†: {provider}\nğŸ“Œ æ¨¡å‹: {model}\nğŸ“Œ æ­£åœ¨å‘é€æµ‹è¯•è¯·æ±‚...'
        else:
            test_start_msg = f'\n{"="*50}\nğŸ” Testing LLM connection...\n{"="*50}\nğŸ“Œ Provider: {provider}\nğŸ“Œ Model: {model}\nğŸ“Œ Sending test request...'
        
        winobj.loglabel.appendPlainText(test_start_msg)
        # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
        winobj.loglabel.verticalScrollBar().setValue(
            winobj.loglabel.verticalScrollBar().maximum()
        )
        
        # å¼ºåˆ¶åˆ·æ–°UI
        from PySide6.QtWidgets import QApplication
        QApplication.processEvents()
        
        # åˆ›å»ºå¹¶å¯åŠ¨æµ‹è¯•çº¿ç¨‹
        test_thread = TestLLMThread(provider, api_key, model, base_url)
        
        def on_test_progress(message):
            """è¿›åº¦æ›´æ–°çš„å›è°ƒ"""
            # åœ¨æ—¥å¿—ä¸­æ˜¾ç¤ºè¿›åº¦
            winobj.loglabel.appendPlainText(f'{message}')
            # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
            winobj.loglabel.verticalScrollBar().setValue(
                winobj.loglabel.verticalScrollBar().maximum()
            )
        
        def on_test_finished(message, success):
            """æµ‹è¯•å®Œæˆçš„å›è°ƒ"""
            # åœ¨æ—¥å¿—ä¸­æ˜¾ç¤ºç»“æœ
            winobj.loglabel.appendPlainText(f'\n{message}')
            winobj.loglabel.appendPlainText(f'{"="*50}\n')
            
            # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
            winobj.loglabel.verticalScrollBar().setValue(
                winobj.loglabel.verticalScrollBar().maximum()
            )
            
            # æ¢å¤æŒ‰é’®çŠ¶æ€
            winobj.llm_test_btn.setDisabled(False)
            winobj.llm_test_btn.setText('ğŸ”\næµ‹è¯•è¿æ¥' if config.defaulelang == 'zh' else 'ğŸ”\nTest\nConnection')
        
        test_thread.progress.connect(on_test_progress)
        test_thread.finished.connect(on_test_finished)
        test_thread.start()
        
        # ä¿å­˜çº¿ç¨‹å¼•ç”¨ï¼Œé¿å…è¢«åƒåœ¾å›æ”¶
        winobj._test_thread = test_thread
    
    def get_file():
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‹–æ”¾æ–‡ä»¶
        if hasattr(winobj.videobtn, 'selected_file') and winobj.videobtn.selected_file:
            fname = winobj.videobtn.selected_file
            winobj.videobtn.selected_file = ""  # æ¸…ç©ºï¼Œé¿å…é‡å¤ä½¿ç”¨
        else:
            # ç‚¹å‡»æŒ‰é’®é€‰æ‹©æ–‡ä»¶
            formats = ['mp4', 'mkv', 'avi', 'mov', 'flv', 'wmv', 'mp3', 'wav', 'flac', 'm4a']
            format_str = ' '.join([f'*.{f}' for f in formats])
            fname, _ = QFileDialog.getOpenFileName(
                winobj, 
                "é€‰æ‹©è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶" if config.defaulelang == 'zh' else 'Select Video/Audio File',
                config.params['last_opendir'],
                f"Video/Audio files({format_str})"
            )
        
        if fname:
            from pathlib import Path
            fname = fname.replace('file:///', '').replace('\\', '/')
            # æ˜¾ç¤ºæ–‡ä»¶å
            file_name = Path(fname).name
            winobj.videoinput.setText(f"âœ… {file_name}\nğŸ“‚ {fname}")
    
    def get_srt_file():
        """é€‰æ‹©å­—å¹•æ–‡ä»¶"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‹–æ”¾æ–‡ä»¶
        if hasattr(winobj.srtbtn, 'selected_file') and winobj.srtbtn.selected_file:
            fname = winobj.srtbtn.selected_file
            winobj.srtbtn.selected_file = ""  # æ¸…ç©ºï¼Œé¿å…é‡å¤ä½¿ç”¨
        else:
            # ç‚¹å‡»æŒ‰é’®é€‰æ‹©æ–‡ä»¶
            fname, _ = QFileDialog.getOpenFileName(
                winobj,
                "é€‰æ‹©å­—å¹•æ–‡ä»¶" if config.defaulelang == 'zh' else 'Select Subtitle File',
                config.params['last_opendir'],
                "Subtitle files(*.srt)"
            )
        
        if fname:
            from pathlib import Path
            fname = fname.replace('file:///', '').replace('\\', '/')
            # æ˜¾ç¤ºæ–‡ä»¶å
            file_name = Path(fname).name
            winobj.srtinput.setText(f"âœ… {file_name}\nğŸ“‚ {fname}")

    def start():
        winobj.has_done = False
        # ä»æ˜¾ç¤ºæ–‡æœ¬ä¸­æå–æ–‡ä»¶è·¯å¾„ï¼ˆæ ¼å¼ï¼šâœ… æ–‡ä»¶å\nğŸ“‚ è·¯å¾„ï¼‰
        video_text = winobj.videoinput.text()
        if 'ğŸ“‚' in video_text:
            video_file = video_text.split('ğŸ“‚')[-1].strip()
        else:
            video_file = video_text
        
        if not video_file or video_file == "æœªé€‰æ‹©æ–‡ä»¶" or video_file == "No file selected":
            tools.show_error(
                'å¿…é¡»é€‰æ‹©è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶' if config.defaulelang == 'zh' else 'Video/audio file must be selected',
                False)
            return
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ LLM
        use_llm = winobj.use_llm_checkbox.isChecked()
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç°æœ‰å­—å¹•
        existing_srt = None
        if winobj.use_existing_srt_checkbox.isChecked():
            # ä»æ˜¾ç¤ºæ–‡æœ¬ä¸­æå–æ–‡ä»¶è·¯å¾„
            srt_text = winobj.srtinput.text()
            if 'ğŸ“‚' in srt_text:
                existing_srt = srt_text.split('ğŸ“‚')[-1].strip()
            else:
                existing_srt = srt_text
            
            if not existing_srt or existing_srt == "æœªé€‰æ‹©å­—å¹•æ–‡ä»¶" or existing_srt == "No subtitle file selected":
                tools.show_error(
                    'è¯·é€‰æ‹©å­—å¹•æ–‡ä»¶' if config.defaulelang == 'zh' else 'Please select subtitle file',
                    False)
                return
            if not Path(existing_srt).exists():
                tools.show_error(
                    'å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨' if config.defaulelang == 'zh' else 'Subtitle file does not exist',
                    False)
                return
        
        # è·å–å‚æ•°
        language = winobj.language_combo.currentText().split('=')[0]
        model_size = winobj.model_combo.currentText()
        
        try:
            max_duration = float(winobj.duration_input.text())
            max_words = int(winobj.words_input.text())
            if max_duration <= 0 or max_words <= 0:
                raise ValueError
        except:
            tools.show_error(
                'å‚æ•°å¿…é¡»æ˜¯æ­£æ•°' if config.defaulelang == 'zh' else 'Parameters must be positive numbers',
                False)
            return
        
        # è·å–è®¾å¤‡é€‰æ‹©
        device = winobj.device_combo.currentText().lower()
        
        # è·å– LLM é…ç½®
        llm_provider = winobj.llm_provider_combo.currentText().lower() if use_llm else ''
        llm_api_key = winobj.llm_api_key_input.text() if use_llm else ''
        llm_model = winobj.llm_model_combo.currentText() if use_llm else ''
        llm_base_url = winobj.llm_base_url_input.text() if use_llm else ''
        
        if use_llm and not llm_api_key and llm_provider != 'local':
            tools.show_error(
                'è¯·è¾“å…¥ LLM API Key' if config.defaulelang == 'zh' else 'Please enter LLM API Key',
                False)
            return

        winobj.startbtn.setText('ç”Ÿæˆä¸­...' if config.defaulelang == 'zh' else 'Generating...')
        winobj.startbtn.setDisabled(True)
        winobj.resultbtn.setDisabled(True)
        winobj.resultinput.setPlainText("")
        winobj.loglabel.setPlainText("ğŸš€ å¼€å§‹å¤„ç†..." if config.defaulelang == 'zh' else 'ğŸš€ Starting...')

        # LLM æ¨¡å¼å¿…é¡»å¯ç”¨
        if not use_llm:
            tools.show_error(
                'æ­¤å·¥å…·å¿…é¡»å¯ç”¨ LLM æ™ºèƒ½æ–­å¥' if config.defaulelang == 'zh' else 'This tool requires LLM smart split',
                False)
            winobj.startbtn.setText('å¼€å§‹ç”Ÿæˆ' if config.defaulelang == 'zh' else 'Start Generate')
            winobj.startbtn.setDisabled(False)
            return
        
        task = LLMSplitThread(
            parent=winobj,
            video_file=video_file,
            language=language,
            model_size=model_size,
            max_duration=max_duration,
            max_words=max_words,
            device=device,
            existing_srt=existing_srt,
            llm_provider=llm_provider,
            llm_api_key=llm_api_key,
            llm_model=llm_model,
            llm_base_url=llm_base_url
        )
        
        task.uito.connect(feed)
        task.start()

    def opendir():
        QDesktopServices.openUrl(QUrl.fromLocalFile(RESULT_DIR))

    from videotrans.component import LLMSplitForm
    try:
        winobj = config.child_forms.get('llmsplitw')
        if winobj is not None:
            winobj.show()
            winobj.raise_()
            winobj.activateWindow()
            return
        winobj = LLMSplitForm()
        config.child_forms['llmsplitw'] = winobj
        
        winobj.videobtn.clicked.connect(get_file)
        winobj.srtbtn.clicked.connect(get_srt_file)
        winobj.use_existing_srt_checkbox.stateChanged.connect(toggle_srt_input)
        winobj.use_llm_checkbox.stateChanged.connect(toggle_llm_settings)
        winobj.llm_test_btn.clicked.connect(test_llm_connection)
        winobj.resultbtn.clicked.connect(opendir)
        winobj.startbtn.clicked.connect(start)
        
        # ç›‘å¬ API Key è¾“å…¥å˜åŒ–ï¼Œè‡ªåŠ¨ä¿å­˜åˆ° .env æ–‡ä»¶
        winobj.llm_api_key_input.textChanged.connect(save_api_key_to_env)
        
        # ç›‘å¬æä¾›å•†å˜åŒ–
        winobj.llm_provider_combo.currentTextChanged.connect(provider_changed_callback)
        
        # ç›‘å¬æ¨¡å‹å’Œ Base URL å˜åŒ–ï¼Œä¿å­˜é…ç½®
        winobj.llm_model_combo.currentTextChanged.connect(save_llm_config)
        winobj.llm_base_url_input.textChanged.connect(save_llm_config)
        
        # åˆå§‹åŒ–æ—¶æ ¹æ®é»˜è®¤çŠ¶æ€æ˜¾ç¤º/éšè—æ§ä»¶
        toggle_llm_settings()
        toggle_srt_input()
        
        # ä» config.params åŠ è½½ä¿å­˜çš„é…ç½®
        saved_provider = config.params.get('llm_provider', 'siliconflow')
        # æŸ¥æ‰¾å¯¹åº”çš„æä¾›å•†å¹¶è®¾ç½®
        for i in range(winobj.llm_provider_combo.count()):
            if winobj.llm_provider_combo.itemText(i).lower() == saved_provider:
                winobj.llm_provider_combo.setCurrentIndex(i)
                break
        
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å– API Key
        load_api_key_from_env()
        
        # åŠ è½½ä¿å­˜çš„æ¨¡å‹
        saved_model = config.params.get('llm_model', 'deepseek-ai/DeepSeek-V3.1-Terminus')
        if saved_model:
            # å¦‚æœæ¨¡å‹ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œæ·»åŠ è¿›å»
            if winobj.llm_model_combo.findText(saved_model) == -1:
                winobj.llm_model_combo.addItem(saved_model)
            winobj.llm_model_combo.setCurrentText(saved_model)
        
        # åŠ è½½ä¿å­˜çš„ Base URL
        saved_base_url = config.params.get('llm_base_url', '')
        if saved_base_url:
            winobj.llm_base_url_input.setText(saved_base_url)
        
        # è®©çª—å£åœ¨å±å¹•ä¸Šå±…ä¸­æ˜¾ç¤º
        from PySide6.QtGui import QGuiApplication
        screen = QGuiApplication.primaryScreen()
        if screen:
            screen_geometry = screen.availableGeometry()
            window_geometry = winobj.frameGeometry()
            center_point = screen_geometry.center()
            window_geometry.moveCenter(center_point)
            winobj.move(window_geometry.topLeft())
        
        winobj.show()
    except Exception as e:
        import traceback
        print(traceback.format_exc())

